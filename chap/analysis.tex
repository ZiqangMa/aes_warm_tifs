\section{Towards the Optimal Performance of {{\scshape{Warm+Delay}}}}
\label{sec:performanceproof}
%\vspace{-1mm}
\def \vdelay {{\scshape{Delay}}}
\def \vwarm {{\scshape{Warm}}}
\def \vwd {{\scshape{Warm+Delay}}}


%In this section, we first introduce five principles that lead to an optimized {\scshape{Warm+Delay}} performance. We then examine these principles with our proposed algorithm, and derive practical conditions which make the proposed algorithm optimal. We first examine the performance of \vdelay, and then analyze the \vwam~operation with AES as the cryptographic algorithm, and show that our scheme is optimal. Last, we discuss the impact of different key lengths and different algorithm implementations.
%All the detailed conditions are practical.


In this section, we prove that the {\scshape{Warm+Delay}} scheme achieves the optimal performance with the least extra operations,
 by applying it to AES and analyzing  the situations that it produces the optimal performance.

\subsection{Overview}
\label{sec:eval}

In the scheme,
        the extra operations are introduced by {\scshape{Warm}} and {\scshape{Delay}},
 so the overheads are determined by the cost of each {\scshape{Warm}} (or {\scshape{Delay}}) operation,
    and the times of these operations.
% so the main point is to optimize the use of {\scshape{Warm}} and {\scshape{Delay}}.
%In the literature, the fundamental rule in defending against timing side channel attacks is to eliminate any observable pattern between the encryption times and the inputs (plaintexts), i.e. to make them fully independent. Three categories of solutions have been proposed~\cite{Braun2015Robust}: (1) application-specific changes, (2) static transformation, and (3) dynamic padding. The first two both require platform-specific modifications to the implementation of the encryption algorithms. We adopt the third approach in our solution (\vdelay), and further introduce {\vwarm}  (i.e., cache warm) to improve performance.
Therefore, the following principles need to be satisfied to achieve the optimal performance:

\begin{itemize}
    \item In the {\vdelay} operation, the imposed delay, or the execution time of padding instructions, is the shortest.
    \item The number of {\vdelay} operations is minimum.
    \item In the {\vwarm} operation, only the minimum necessary data are loaded into caches. % as little as possible.
    \item The number of  {\vwarm} operations is minimum.
    \item Because {\vwarm} is much more efficient than {\vdelay},{\footnote{It is true for any computing platform in practice; otherwise, caches are useless in performance improvement.}} {\vwarm} is preferred over {\vdelay}. % than \verb+delay+.
\end{itemize}

Next, we examine these principles with the {\vwd} scheme, and derive the practical conditions which make it optimal.
We first examine the performance of \vdelay, and then analyze the {\vwarm} operation for AES-128 with 2KB lookup tables \cite{openssl}.
In the proof, we show that it is statistically the most efficient to load all lookup tables in \vwarm, instead of loading parts of the tables (which probably leads to more {\vdelay} in the future).
Then we identify the best condition for the {\vwarm} operation,
  and eventually derive the practical condition so that the {\vwd} scheme (with appropriate parameters) is optimal.
%That is, performing {\vwarm} when the execution time of encryption lies in $(T_{NM}, \infty)$ is the optimal in commodity systems.

Last, we extend the conclusions to different key lengths of AES and different implementations,
  and show that, the conditions are applicable to these typical key lengths and implementations.
That is, the derived {\scshape{Warm+Delay}} scheme achieves the optimal performance for various AES implementations with lookup tables.

%In our {\vwd}  scheme, we perform delay operation when the execution time is in $(T_{NM}, T_{W})$, and delay it to $T_{W}$. The {\vwarm}  (cache warm) operation in our scheme loads all the lookup tables into the cache. Hence, the overhead depends on the size of the lookup tables and the implementation of algorithms. We implement our scheme in AES-128 with 4.25KB lookup tables in the proof in Section 4.3. In the proof, we show that it is statistically the most efficient to load all lookup tables in \vwarm, instead of loading parts of the tables (which leads to more {\vwarm}  in the future). Then we identify the best timing for the {\vwarm}  operation. In particular, we will discuss three cache-access states: (1) all the lookup tables are in the cache ($S_{full}$); (2) the lookup entries not in the cache are not accessed ($S_{nonas}$); (3) and the entries not in the cache are accessed ($S_{as}$). We will compare the additional time introduced by our scheme vs. other methods. Eventually, we derive a practical condition, so that our scheme is optimal when the condition is satisfied. That is, performing warmup operation when the execution time lies in $(T_{NM}, \infty)$ is the optimal method in our experiment environment.

%The sketch is needed here!+++++++++++

%Next we discuss \verb+delay+ and \verb+warmup+ in details. we discuss the \verb+warmup+ operation after the \verb+delay+ operation because the \verb+warmup+ has some relation to the \verb+delay+.

%\begin{CJK}{UTF8}{gkai}
%最优化证明的目的在于，证明我们的方案是最优的，而保证最优的原则有以下几点：
%1、	warm内容尽可能少
%2、	warm的次数尽可能少
%3、	delay的时间尽可能短
%4、	delay的次数尽可能少
%5、 原则上由于delay时间大于warm时间，因此尽可能多的使用warm，和尽可能少的使用delay
%然后对warm 和delay 分别进行详细证明。
%\end{CJK}

\subsection{Performance Analysis of \vdelay}
%\vspace{-2mm}
%The analysis of {\vdelay}  is independent of the encryption algorithm used in the scheme.
\begin{theorem}
\label{delay2tw} In the {\vdelay} operation, delay to $T_{W}$ imposes the minimal overhead
   while effectively eliminates the cache timing side channels.
\end{theorem}

\begin{IEEEproof}
From the attackers' point of view,
   there are three types of measured time that are unexploitable
    (i.e. the execution time does not reflect the cache misses/hits of \emph{specific} lookup table entries):
    $T_{NM}$, $T_{W}$, and any value greater than $T_{W}$.
$T_{NM}$  means that \emph{all} accessed entries are in caches before encryption,
 and
$T_{W}$ corresponds to the longest execution path, when all lookup tables are not cached before encryption (and they are loaded into caches as the encryption operation is performed);
 so such results are the same for any input.
When the measured time is longer than $T_{W}$,
  encryption is disturbed by unknown system activities (e.g. interrupts and task scheduling),
   and it is impossible for the attackers to exclude the disturbance and find the exact execution time of encryption.

If we delay the execution time to any value less than $T_{W}$,
 a little information about the cache access leaks.
Or, if we delay it to a random value, which may be greater or less than $T_{W}$,
    the attackers could exclude all results greater than $T_{W}$
      and the other results are still exploitable.
Such methods reduce the attack accuracy on each invocation,
  but does not eliminate the cache timing side channels completely.
Therefore, delay to $T_{W}$ imposes the minimal overhead
   while effectively eliminates the cache timing side channels.
%\qed
\end{IEEEproof}


\begin{theorem} Performing the {\vdelay} operation when the execution time of encryption
 is in $(T_{NM}, T_{W})$ results in the smallest number of {\vdelay} operations. % when we cannot distinguish the reason why encryption takes longer than $T_{NM}$ (cache miss or the OS scheduler and  interrupt handler).
\end{theorem} % 这里when后的部分需要吗
%******BL: 我也觉得后面半句可以不要，因为下文有解释。我先把这半句注释掉了


\begin{IEEEproof}
As described above,
 if the execution time of encryption is equal to or less than $T_{NM}$, no cache miss occurs,
  and
if the execution time is equal to $T_W$ or greater, it is unexploitable.
The execution time is independent of keys and plaintexts/ciphertexts in these cases,
  so {\vdelay} is unnecessary.


%If the encryption time is greater than $T_{W}$,
% it means that the OS scheduler or interruption handler has been invoked during encryption -- the attacker could not recover the actual encryption time to perform timing attacks, because the time for OS scheduler or interruption handler is unpredictable.

When the execution time is in $(T_{NM}, T_{W})$,
 it may be due to cache misses, and/or system activities (but the overhead is less than $T_{W} - T_{NM}$).
We cannot distinguish the exact reasons without special privileges.
However, the attackers can distinguish them by repeatedly invoking the cryptographic function using the same input (and secret key).
 Therefore, {\vdelay} is  necessary in this case.
%\qed
\end{IEEEproof}

\subsection{Performance Analysis of \vwarm}
%\vspace{-1mm}

We apply the {\vwd}  scheme to AES-128 with 2KB lookup tables \cite{openssl},
  as described in Section \ref{back:aes}.
%The {\vwarm}  operation is applied to the lookup tables, which consume 2KB in total.
We show that, in this case, the {\scshape{Warm+Delay}} scheme produces the best performance in commodity computer systems,
that is, introduces the least extra {\vwarm} and {\vdelay} operations.
The details about other key sizes (192 and 256 bits) and different implementations (4KB, 4.25KB, and 5KB lookup tables),
  are included in Appendix \ref{appendixb}.
%In Section \ref{analy:otherkey}, we also discuss other key lengths and other implementations and other algorithms.

Denote the size of a cache line as $C$,
    and the time cost to load a cache line of data from RAM to L1D caches as $T_{cl}$.
  So we have the following theorem,
  for an AES implementation of $R$ rounds with an $L$-byte lookup table ($L \gg C$).

%The average time saved by not loading one cache line from RAM is denoted as $t_{nc}$,


%\begin{theorem}\label{theorm:warmall} For the hardware of commodity computers, loading all the AES lookup tables into the cache is the best warm operation.\end {theorem}
\begin{theorem}\label{theorm:warmall}
If $B_{nl}(N) < D_{nl}(N)$ for any $0 < N \leq \frac{L}{C}$,
%\Delta T_{warm} < E(T_{delay})$ ,
 loading all entries of the lookup table into caches in the {\vwarm} operation provides better performance
 than any part of entries,
where $B_{nl}(N) = N T_{cl}$ and $D_{nl}(N) = (1- (1-\frac{NC}{L})^{16 R})(T_{W}-T_{NM})$.
\end{theorem}
%\begin{theorem}\label{theorm:warmall}
%If $T_{cl} < (1- (1-\frac{NC}{L})^{16 R})(T_{W}-T_{NM})/N$ for any $0 < N \leq \frac{L}{C}$,
% then loading all entries of the lookup table into caches in the {\vwarm} operation provides better performance
% than any part of entries.
%%where $B_{nl}(N) = N T_{cl}$ and $D_{nl}(N) = (1- (1-\frac{NC}{L})^{16 R})(T_{W}-T_{NM})$.
%\end{theorem}

\begin{IEEEproof}
It takes $T_{cl} > 0$ to load data into a cache line in the {\vwarm} operation;
 on the other hand,
 if some entry is uncached,
  the execution time of AES encryption may become longer and then it triggers an extra {\vdelay} operation.
Not loading $N$ ($\frac{L}{C} \geq N > 0$) cache lines of table entries saves the execution time of \vwarm,
 while the potential cost is
    the longer execution of encryption  and the extra execution of \vdelay.

Since $L \gg C$, we needs $\frac{L}{C}$ cache lines to hold the lookup table.
In each round of AES encryption,
 the lookup table is accessed for $16$ times.
We assume that, in each round, the input of \texttt{SubBytes} is random and then
    each table entry is accessed  uniformly.
Hence,
 the probability that $N$ certain cache lines are not accessed after $R$ rounds of AES encryption (denoted as $P_{N}$),
  is $P_N = (1-\frac{NC}{L})^{16 R}$.



%In AES, {\vwarm}  loads all lookup tables into cache. In the case that at least one corresponding cache line is \emph{not} loaded,
% cache miss may happen, which will in turn trigger {\vdelay}  operation. Therefore,

%For {\vwarm} operation, if there exists $N>0$ making the time saved by not warming $N$ cache lines larger than the expected overhead of invoking {\vdelay}, not warming $N$ cache lines provides better performance. Otherwise, loading all lookup tables into the cache is the best.


%For AES with 2KB lookup table implementation, it just use one lookup table $L_{2k}$ with 2KB.
%For a cache line utilized by AES look up tables, the probability that it loads $T_{0}, T_{1}, T_{2}, T_{3}$ is $16/17$, while the probability it loads the content of $T_{4}$ is $1/17$.

%Therefore, the probability that a certain cache line of $L_{2k}$ is not accessed in all rounds is $P_{1} = (1-\frac{C}{L})^{160}$.
%The expected probability that any cache line is not accessed in one AES execution is $P_{e} = \frac{16}{17}P_{0-3}+ \frac{1}{17}P_{4}$.
If a table entry is uncached but accessed during the execution of AES encryption,
   the execution time is greater than $T_{NM}$ and {\vdelay} is performed.
Therefore, if $N$ cache lines of table entries are not in caches,
    the probability of extra {\vdelay} is listed as follows.
  \begin{equation}
  \label{equ:delay}
 \bar{P}_N = 1-P_N = 1- (1-\frac{NC}{L})^{16 R}
\end{equation}


The execution time shall be delayed to $T_W$ according to Theorem \ref{delay2tw},
 while it is $T_{NM}$ if all entries are in caches.
So the expected overhead (or the time cost) due to not loading $N$ cache lines of table entries is
\begin{eqnarray}
 \label{equ:tdelay}
&D_{nl}(N)&={ }  \bar{P}_N * (T_{W}-T_{NM})  \nonumber \\
&&= (1- (1-\frac{NC}{L})^{16 R})(T_{W}-T_{NM})
\end{eqnarray}

Finally,
the benefit  of not loading data into $N$ cache lines in the {\vwarm} operation is
\begin{eqnarray}
B_{nl}(N) = N T_{cl}
\end{eqnarray}

Finally,
(1)
If $B_{nl}(N) < D_{nl}(N)$ for any $\frac{L}{C} \geq N > 0$,
 loading all entries of the lookup table into caches in the {\vwarm} operation provides the optimal performance;
% (1) If $\forall N>0, \Delta T_{warm} < E(T_{delay})$, warming all cache lines is always better.
and (2) if there exists $N$ satisfying that $B_{nl}(N) > D_{nl}(N)$, not loading $N$ cache lines of table entries provides better performance.
%  and (3) If $\exists N>0, \Delta T_{warm} = E(T_{delay})$, warming or not warming $N$ cache lines provides equivalent performance.
%\qed
\end{IEEEproof}


In the {\scshape{Warm+Delay}} scheme, {\vwarm} is performed  after encryption
    as a part of the {\vdelay} operation, as shown in Algorithm \ref{alg:aesdefense}.
So the \emph{actual} cost of loading data into $N$ cache lines (or
   the benefit of not loading these data) in {\vwarm} is less than $B_{nl}(N)$.
For example,
    when the time of {\scshape{Crypt}} plus {\vwarm} is less than  $T_W$ (Line 8 in Algorithm \ref{alg:aesdefense}),
  the  cost of {\vwarm} is actually zero.

Therefore,
  the average cost of loading data into $N$ cache lines in the {\vwarm} operation is \emph{actually} less than $N T_{cl}$,
      so loading all entries in {\vwarm} still provides better performance
 than any part of entries if $B_{nl}(N) = N T_{cl} < D_{nl}(N)$ for any $\frac{L}{C} \geq N > 0$.


% Therefore, the time of {\vwarm}  becomes part of delay.
% When this case occurs, the extra time reduced by warming some cache line is zero,
% if the AES execution time plus {\vwarm}  time is less than $T_{W}$.


% 这一段我是想说，咱们的方案中，warm 是在AES 计算之后进行的，当AES计算如果大于WET的时候，warm之后不进行delay，所以warm少几个cache line节省的开销是按照上面的计算，说明warm 比不warm 要优。而在AES计算小于WET 时，warm操作本身就是delay的一部分，那么可以认为warm的操作开销减小，这时warm 自然是全部warm 最优。

\begin{corollary}

For the AES-128 implementation with a 2KB lookup table
in commodity computer systems,
loading all entries of the lookup table into caches in the {\vwarm} operation
   provides better performance than any part of entries.
\end{corollary}

\begin{IEEEproof}
Firstly,
 with commodity hardware, the cache is enough to load all entries of the lookup table.
For example, AES is implemented with the lookup table of 2KB, 4KB, 4.25KB or 5KB,
   and the lookup table of DES/3DES is 2KB.
However, the typical L1D cache is 32KB or 64KB for Intel CPUs, and 16KB or 32KB for ARM CPUs.


In our experiments on a Lenovo ThinkCentre M8400t PC with an Intel Core i7-2600 CPU and 2GB RAM,
$T_{W}$, $T_{NM}$, and $T_{cl}$ are {\color{red}2834, 309, 55.68} in CPU cycles, respectively
 (see Section \ref{sec:implementation} for details).
Table~\ref{tbl:warmcontent} shows $B_{nl}(N)$ and $D_{nl}(N)$,
    when $R = 10$, $L = 2KB$, and $C = 64B$.
We find that, the cost of not loading $N$ ($32 \geq N >0$) cache lines is always much greater than the benefit.
The result is applicable to other commodity computer systems.
So  loading all entries in {\vwarm} produces the optimal performance.
\end{IEEEproof}


 %The time $T_{W}$ is 2834 CPU cycles, while the shortest AES execution time $T_{NM}$ is 309 cycles. The average time $T_{cl}$ for loading one cache line of lookup table is 55.68 cycles.

\begin{table*}[t]
%    \renewcommand{\arraystretch}{1.3}
%    \caption{A Simple Example Table}
%    \label{table_example}
%    \centering
%    \begin{tabular}{c||c}
%    \hline
%    \bfseries First & \bfseries Next\\
%    \hline\hline
%    1.0 & 2.0\\
%    \hline
%    \end{tabular}
%    \end{table}
  \renewcommand{\arraystretch}{1.3}
  \caption{Benefit and expected cost of not loading $N$ cache lines of table entries (in CPU cycles).}\label{tbl:warmcontent}
  \centering
   \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
   \hline
   {$N$}           & 1      & 2  & 3    & 5   & 10   & 15   & 20  & 25   & 30  & 31  & 32\\
   \hline
   {$B_{nl}(N)$} & 55.68 & 111.35 & 167.03 & 278.38 & 556.75 & 835.13 & 1113.5s & 1391.88 & 1670.25 & 1725.93 & 1781.60 \\
   \hline
   {$D_{nl}(N)$} & 2509.29 & 2524.92 & 2524.99 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 \\
   \hline
   \end{tabular}
\end{table*}


%For example, L1D xxx, AES xx KB, 3DES xx KB
%How about Intel CPU, ARM CPU? L1D cache size?

%************************************分隔符******************************************
%\begin{theorem}\label{lemma:warmaftermiss}
%For commodity computers, performing  {\vwarm}  when cache-miss occurred during the previous execution results the minimum extra time.
%\end{theorem}

Next, let's consider the different cache state of lookup tables during the continuous invocations of AES encryption.
For each execution of AES encryption,
  and there are three possible states:
(1) all entries of the lookup table are in caches, denoted as $S_{c}$,
   (2) some entries are uncached, and at least one of them is accessed during the AES encryption, denoted as $S_{\bar{c}, a}$,
 and  (3) some entries are uncached, but none of them is accessed during the AES encryption, denoted as $S_{\bar{c}, \bar{a}}$.
$S_{\bar{c}, a}$ and $S_{\bar{c}, \bar{a}}$ represent the states where one or more cache lines of table entries are uncached or evicted from caches.


]]]]]]]]

If task scheduling, interrupts or any other system activities occur during the execution of encryption,
 the entries of lookup tables may be evicted from caches, and the probability is denoted as $P_{evict}$.
For one AES encryption, some entries are uncached before encryption and it is possible that none of them is accessed during the encryption, and this probability is denoted as $P_{\bar{c},\bar{a}}$.
{\color{red} does the definition mean, `provided that some entries are uncached', the probability that they are unaccessed?
if yes, we shall denote it as $P_{\bar{a}ccess}$}.

% 哪些事件导致状态转移？概率表示为。
% 事件的定义
%warm:
%evict:
%access: P   $\bar{P}$

%$P_{nona}$ (equals to $P_{N}$) is the probability that the entries not in the cache are not needed in one execution; while $P_{evict}$ is the probability that some entries are evicted from the cache.

Denote the execution time of {\vdelay} and {\vwarm} as
    $T_{delay}$ and $T_{warm}$, respectively.
$T_{delay}$ is constant for different executions,
    while $T_{warm}$ depends on the number of loaded cache lines of table entries.

In {\vwd} scheme, the warm strategy that perform {\vwarm} when some cache misses occur during the pervious execution of encryption is denoted as conditional {\vwarm}.
While considering other two cases: (1)performing {\vwarm} with a probability $P_{Lwarm}\in[0,1)$  when a cache miss occurs during the pervious execution of encryption; (2)performing {\vwarm} when some cache misses occur during the pervious execution of encryption and also performing {\vwarm} with a probability $P_{Mwarm}\in[0,1]$ in other conditions. These two cases are denoted as less {\vwarm} and more {\vwarm} respectively.

\begin{figure*}[t]
    \centering
    \subfloat[Conditional warm]{\includegraphics[width=0.33\textwidth]{pic/conditionwarm.pdf}
    \label{fig:conditioncase}}
    \hfil
    \subfloat[Less warm]{\includegraphics[width=0.5\textwidth]{pic/probwarmless.pdf}
    \label{fig:lesscase}}
    \hfil
    \subfloat[More warm]{\includegraphics[width=0.5\textwidth]{pic/probwarmmore.pdf}
    \label{fig:morecase}}
    \caption{The Markov state-transferring probability diagram.}
    \label{pic:statetrans}

\end{figure*}

For conditional {\vwarm}, the Markov state transition diagram is in \figurename~\ref{fig:conditioncase}.
We use $\Pi_{c}^{C}$, $\Pi_{\bar{c}, \bar{a}}^{C}$, $\Pi_{\bar{c}, a}^{C}$ to represent the limit distribution of the three states  when the Markov chain in stable state, and the values are as follows.
%\setlength{\arraycolsep}{0.0em}
\begin{eqnarray}
\label{equ:pic}
&&\Pi_{c}^{C} = \frac{P_{a2}}{P_{a2}+P_{evict}P_{a2}+P_{evict}(1-P_{a1})}\ , \nonumber\\
&&\Pi_{\bar{c}, \bar{a}}^{C} = \frac{P_{evict}(1-P_{a1})}{P_{a2}+P_{evict}P_{a2}+P_{evict}(1-P_{a1})}\ , \\
&&\Pi_{\bar{c}, a}^{C} = \frac{P_{evict}P_{a2}}{P_{a2}+P_{evict}P_{a2}+P_{evict}(1-P_{a1})} .  \nonumber
\end{eqnarray}
%\setlength{\arraycolsep}{5pt}

For less {\vwarm}, the Markov state transition diagram is in \figurename~\ref{fig:lesscase}.
We use $\Pi_{c}^{L}$, $\Pi_{\bar{c}, \bar{a}}^{L}$, $\Pi_{\bar{c}, a}^{L}$ to represent the limit distribution of the three states  when the Markov chain in stable state, and the values are as follows.
%\setlength{\arraycolsep}{0.0em}
\begin{align}
\label{equ:pic}
&\Pi_{c}^{L} = \frac{P_{a1}P_{a2}^{'}}{P_{a1}P_{a2}^{'}+P_{evict}P_{Lwarm}(1-P_{a1})+P_{evict}(1-P_{Lwarm})(1-P_{a3})+P_{evict}P_{a2}^{'}}\ , \nonumber\\
&\Pi_{\bar{c}, \bar{a}}^{L} = \frac{P_{evict}P_{Lwarm}(1-P_{a1})+P_{evict}(1-P_{Lwarm})(1-P_{a3})}{P_{a1}P_{a2}^{'}+P_{evict}P_{Lwarm}(1-P_{a1})+P_{evict}(1-P_{Lwarm})(1-P_{a3})+P_{evict}P_{a2}^{'}}\ , \\
&\Pi_{\bar{c}, a}^{L} = \frac{P_{evict}P_{a2}^{'}}{P_{a1}P_{a2}^{'}+P_{evict}P_{Lwarm}(1-P_{a1})+P_{evict}(1-P_{Lwarm})(1-P_{a3})+P_{evict}P_{a2}^{'}} .  \nonumber
\end{align}
%\setlength{\arraycolsep}{5pt}

For more {\vwarm}, the Markov state transition diagram is in \figurename~\ref{fig:morecase}.
We use $\Pi_{c}^{M}$, $\Pi_{\bar{c}, \bar{a}}^{M}$, $\Pi_{\bar{c}, a}^{M}$ to represent the limit distribution of the three states  when the Markov chain in stable state, and the values are as follows.
%\setlength{\arraycolsep}{0.0em}
\begin{align}
\label{equ:pic}
&\Pi_{c}^{M} = \frac{1-(1-P_{Mwarm})(1-P_{a2}^{''})}{(1+P_{evict})(1-(1-P_{Mwarm})(1-P_{a2}^{''}))+P_{evict}(1-P_{Mwarm})(1-P_{a1})}\ , \nonumber\\
&\Pi_{\bar{c}, \bar{a}}^{M} = \frac{P_{evict}(1-P_{a1})}{(1+P_{evict})(1-(1-P_{Mwarm})(1-P_{a2}^{''}))+P_{evict}(1-P_{Mwarm})(1-P_{a1})}\ , \\
&\Pi_{\bar{c}, a}^{M} = \frac{P_{evict}(1-(1-P_{Mwarm})(1-P_{a2}^{''}))-P_{Mwarm}P_{evict}(1-P_{a1})}{(1+P_{evict})(1-(1-P_{Mwarm})(1-P_{a2}^{''}))+P_{evict}(1-P_{Mwarm})(1-P_{a1})} .  \nonumber
\end{align}
%\setlength{\arraycolsep}{5pt}

Then we have the following theorems.


\begin{theorem}\label{lemma:warmnum1}
  The conditional {\vwarm} strategy has better performance than the less {\vwarm} strategy.
\end{theorem}
\begin{IEEEproof}
The expected extra time introduced by less {\vwarm} and conditional {\vwarm}  are denoted as $E(T^{L})$ and $E(T^{C})$, respectively.
So,
%\begin{footnotesize}
\begin{equation}
\label{equ:tpc1}
E(T^{L})-E(T^{C})=\Pi_{\bar{c}, a}^{L} * T_{delay} - \Pi_{\bar{c}, a}^{C} * T_{delay}
\end{equation}

And then, we get that $E(T^{L}) > E(T^{C})$ for all $P_{Lwarm}\in(0,1)$.
So the extra time introduced by less {\vwarm} strategy is always larger than conditional {\vwarm} strategy. That is the conditional {\vwarm} strategy has better performance than the less {\vwarm} strategy.
\end{IEEEproof}


Let $g = T_{delay} / T_{warm}$ and $M = T_{warm_{min}} / T_{delay}$.

\begin{theorem}\label{lemma:warmnum2}
If $P_{evict}\leq \frac{M*P_{a2}}{1-P_{a1}}$,
    conditional {\vwarm} strategy has better performance than the less {\vwarm} strategy;
or if $P_{evict} > \frac{M*P_{a2}}{1-P_{a1}}$,
    conditional {\vwarm} strategy has better performance than the less {\vwarm} strategy when $g<g_{max}$,
        where
\begin{equation}
\label{equ:kzero}
g_{max} = \frac{P_{evict}(1-P_{a1})(P_{a2}+P_{evict}P_{a2}+P_{evict}(1-P_{a1}))}{(P_{evict}P_{a2}+P_{a2}-P_{evict}P_{a1})(P_{evict}(1-P_{a1})-MP_{a2})-MP_{evict}P_{a2}}
\end{equation}
 \end{theorem}

\begin{IEEEproof}
The expected extra time introduced by more {\vwarm} and conditional {\vwarm}  are denoted as $E(T^{M})$ and $E(T^{C})$, respectively.
Then,
%\begin{footnotesize}
\begin{align}
\label{equ:tpc1}
E(T^{M})-E(T^{C})&=\Pi_{\bar{c}, a}^{M} * T_{delay} + \Pi_{c}^{M}*P_{warm}*T_{warm_{min}} \nonumber \\
 &+ \Pi_{\bar{c}, \bar{a}}^{M}*P_{warm}*T_{warm} - \Pi_{\bar{c}, a}^{C} * T_{delay}
\end{align}

So we get that: (1)when $P_{evict} \leq \frac{M*P_{a2}}{1-P_{a1}}$, the formula $E(T^{M})-E(T^{C}) \geq 0$ holds;
(2)when $P_{evict} > \frac{M*P_{a2}}{1-P_{a1}}$, the formula $E(T^{M})-E(T^{C}) \geq 0$ holds if $g<g_{max}$.
Therefore in these condition conditional {\vwarm} strategy has better performance than the less {\vwarm} strategy.
\end{IEEEproof}

\begin{theorem}\label{lemma:warmnum3}
If $P_{evict}\leq \frac{M*P_{a2}}{1-P_{a1}}$,
or if $P_{evict} > \frac{M*P_{a2}}{1-P_{a1}}$ and $g<g_{max}$,
    performing {\vwarm} when some cache misses occur during the previous execution of encryption results in the least extra time of {\vwarm} and {\vdelay}.
\end{theorem}

\begin{IEEEproof}
Firstly, we consider an extreme case that is performing {\vwarm} with a probability $P_{Cowarm}\in[0,1]$ just when cache misses do not occur during the pervious execution of encryption and not performing {\vwarm} when cache misses occur. This case is denoted as complementary {\vwarm}.
In this case, when there is a cache miss in the previous, {\vdelay} is needed. If the next encryption accesses any entry that is currently not in caches, performing {\vwarm} before hand introduces non extra overhead. However, if not performing {\vwarm}, as proved in Theorem~\ref{theorm:warmall}, when some cache line size of lookup tables are not cached, not performing {\vwarm} introduces more extra time.
 Further more, complementary {\vwarm} strategy will introduce extra overhead when cache misses no occur. Therefore complementary {\vwarm} strategy has less performance than conditional {\vwarm}.
On the other hand, the Markov state transition diagram of complementary {\vwarm} is in \figurename~\ref{fig:morecase}.
Using the same calculation process within Theorem~\ref{lemma:warmnum1},
 we can get that conditional {\vwarm} strategy has better performance than complementary {\vwarm} strategy.

Generally,
 arbitrary warm strategies can be divided into the combination of less {\vwarm}, more {\vwarm} and complementary {\vwarm}.
  We denote the general warm strategy as $W$. In $W$, performing {\vwarm} with a probability $P_{1}\in[0,1)$  when a cache miss occurs during the pervious execution of encryption while performing {\vwarm} with a probability $P_{2}\in[0,1]$ in other conditions.
   Similarly, we denote less {\vwarm} strategy, more {\vwarm} strategy and complementary {\vwarm} as $W_{L}$, $W_{M}$ and $W_{Co}$.
  So we can get the equation:
  \begin{equation}
W = x_{1}W_{L}+x_{2}W_{M}+x_{3}W_{Co}
\end{equation}
  where $x_{1}$, $x_{2}$ and $x_{3}$ satisfy:
$$\left\{
\begin{aligned}
&x_{2}P_{Mwarm}+x_{3}P_{Cowarm}=P_{2} \\
&x_{1}P_{Lwarm}+x_{2}=P_{1} \\
&x_{1}+x_{2}+x_{3}=1
\end{aligned}
\right.
$$
In this way, the generate warm strategy is divided. It is already proved that conditional {\vwarm} has the best performance compared with less {\vwarm} strategy, more {\vwarm} strategy and complementary {\vwarm} if $P_{evict}\leq \frac{M*P_{a2}}{1-P_{a1}}$,
or if $P_{evict} > \frac{M*P_{a2}}{1-P_{a1}}$ and $g<g_{max}$.
Therefore, with this condition, conditional {\vwarm} has better performance than generate warm strategy for any $P_{1}$ and $P_{2}$.
That is  performing {\vwarm} when some cache misses occur during the previous execution of encryption results in the least extra time of {\vwarm} and {\vdelay}.
\end{IEEEproof}

%\begin{IEEEproof}
%%We cannot directly determine or predict whether the lookup tables are in the cache or not. Therefore, we can only decide whether to perform the {\vwarm}  operation based on the time of the previous execution. %As we just proved, whenever we perform \vwarm, we will load all lookup tables into cache.
%In order to prove Theorem \ref{lemma:warmaftermiss}, we compare it with two cases:
%\begin{itemize}
%  \item When there is a cache miss in the previous AES execution, we perform  {\vwarm}  with a probability less than 1.
%  \item Performing {\vwarm} operation with a probability $P_{warm}$, regardless of the cache state after the previous execution.
%\end{itemize}
%
% Denote this strategy as \emph{probabilistic \vwarm}, while our scheme as \emph{conditional \vwarm}.
%
%
%We use the Markov model to analyse both \emph{probabilistic \vwarm}~and \emph{conditional \vwarm}.
%\figurename~\ref{pic:statetrans} shown the state transition diagram.
%We use $\Pi_{full}^{p}$ ($\Pi_{full}^{c}$), $\Pi_{nonas}^{p}$ ($\Pi_{nonas}^{c}$), $\Pi_{as}^{p}$ ($\Pi_{as}^{c}$) to represent the limit distribution of the three states  when the Markov chain in stable state for the probabilistic and conditional {\vwarm}  respectively, and the values are as follows.
%
%%\begin{figure*}[t]
%%    \centering
%%    \subfloat[Our scheme]{\includegraphics[width=0.33\textwidth]{pic/conditionwarm.pdf}
%%    \label{fig:conditioncase}}
%%    \hfil
%%    \subfloat[Less warm]{\includegraphics[width=0.5\textwidth]{pic/probwarmless.pdf}
%%    \label{fig:lesscase}}
%%    \hfil
%%    \subfloat[More warm]{\includegraphics[width=0.5\textwidth]{pic/probwarmmore.pdf}
%%    \label{fig:morecase}}
%%    \caption{The Markov state-transferring probability diagram.}
%%    \label{pic:statetrans}
%%
%%\end{figure*}
%
%%%\begin{footnotesize}
%%\setlength{\arraycolsep}{0.0em}
%%\begin{eqnarray}
%%\label{equ:pip}
%%%\scriptsize
%%&\Pi_{full}^{p} = \frac{P_{warm}}{P_{evict}+P_{warm}}\ , \Pi_{nonas}^{p} = \frac{P_{nona}P_{evict}}{P_{evict}+P_{warm}}\ , \nonumber\\
%%&\Pi_{as}^{p} = \frac{P_{evict}(1-P_{nona})}{P_{evict}+P_{warm}}
%%\end{eqnarray}
%%\begin{eqnarray}
%%\label{equ:pic}
%%%\scriptsize
%%&\Pi_{full}^{c} = \frac{1-P_{nona}}{1-P_{nona}+P_{evict}}\ , \Pi_{nonas}^{c} = \frac{P_{nona}P_{evict}}{1-P_{nona}+P_{evict}}\ , \nonumber\\
%%&\Pi_{as}^{c} = \frac{P_{evict}(1-P_{nona})}{1-P_{nona}+P_{evict}}
%%\end{eqnarray}
%%\setlength{\arraycolsep}{5pt}
%%%\end{footnotesize}
%
%The expected extra time introduced by the probabilistic and conditional {\vwarm}  are denoted as $E(T^{p})$ and $E(T^{c})$, respectively.
%Then,
%%\begin{footnotesize}
%\begin{eqnarray}
%\label{equ:tpc1}
%E(T^{p})-E(T^{c})&=&\Pi_{as}^{p} * t_{delay} - \Pi_{as}^{c} * t_{delay} \nonumber \\
%&& + (1 - \Pi_{as}^{p})*P_{warm} *t_{warm}
%\end{eqnarray}
%%\end{footnotesize}
%%\begin{equation}\begin{split}
%%\label{equ:tpc2}
%%\scriptsize
%%T^{p} - T^{c} =& \frac{1}{(P_{warm}+P_{evict})(P_{delay}+P_{evict})}((P_{delay}+P_{evict})*P_{warm}^{2} + \\ & P_{evict}*(P_{delay}+P_{evict})(1-P_{delay})*P_{warm} -\\ & k*P_{delay}*P_{evict}*P_{warm}+ k*P_{delay}^{2}*P_{evict})t_{warm}
%%\end{split}\end{equation}%这个公式太长了，是不是可以在这里不写，放到附录里？
%
%We use so $E(T^{p}) > E(T^{c})$ holds when  $1 \leq k \leq k_{0}$. %from the equation we can get that when
%%$$
%%\label{equ:kres}
%%    1 \leq k \leq k_{0}
%%$$
%
%\end{IEEEproof}

%$T^{p} > T^{C}$.
\begin{corollary}
For the AES-128 implementation with a 2KB lookup table
in commodity computer systems,
performing  {\vwarm}  when cache-miss occurs during the previous execution,
  results in the least extra time of {\vdelay} and {\vwarm}.
\end{corollary}

\begin{IEEEproof}
For the AES-128 implementation with a 2KB lookup table, $P_{a1}$ is larger than 0.994 from Equation~\ref{equ:delay}.
The range of $P_{a2}$ is $[0.994,1]$.
For the Lenovo ThinkCentre M8400t PC with an Intel Core i7-2600 CPU and 2GB RAM,
$T_{delay}$ is $2525.00$ cycles;
the minimum of $T_{warm}$($T_{warm_{min}}$) is $124$ cycles when accessing the lookup table from L1D caches;
and the maximum of $T_{warm}$ is $1781.60$ cycles when all the lookup tables in RAM.
Thus, $M=0.0491$ and the rang of $g$ is $[1.417,20.363]$.



In Equation~\ref{equ:kzero}, if we regard $g_{max}$ as the function of $P_{a1}$, we can get that $g_{max}$ is a monotony increase function when $P_{a1}$ is larger than 0.994.  
That is when $P_{a1} = 0.994$, $g_{max}$ gets the minimum value.
Also if we regard $g_{max}$ as the function of $P_{evict}$, we can get that $g_{max}$ is a monotony decrease function when when $P_{a1}$ is larger than 0.994.
In the same way, if we regard $g_{max}$ as the function of $P_{a2}$, when $P_{evict}$ is small, $g_{max}$ is a monotony increase function. But when $P_{evict}$ is large, $g_{max}$ is a monotony decrease function.
Therefore, the minimum value of $g_{max}$ is 40.3 when $P_{evict} = 1$, $P_{a1} = 0.994$ and $P_{a2} = 1$.
The max value of $g$ is less than the minimum value of $g_{max}$, that is $g < g_{max}$.
So according to Theorem~\ref{lemma:warmnum3}, performing  {\vwarm}  when cache-miss occurs during the previous execution results the minimum extra time
\end{IEEEproof}
%\qed
%\end{IEEEproof}


%\begin{table*}[t]%[!hbt]
%  \centering
%  \small
%  \caption{The values of $k_{0}$ corresponding to $P_{evict}$. ($P_{delay} = 0.994$)}\label{tbl:kvalue}
%   \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
%   \hline
%   {$P_{evict}$} & 0   & 0.05  & 0.1 & 0.2   & 0.3   & 0.4   & 0.5  & 0.6   & 0.7    & 0.8  & 0.9 & 1 \\
%   \hline
%   {$k_{0}$} & $\infty$ & 3502.1 & 1835.4 & 1002.2 & 724.5 & 585.7 & 502.5 & 447.1 & 407.5 & 377.8 & 354.8 &336.3 \\
%   \hline
%   \end{tabular}
%\end{table*}

%In the above proof, we treated AES as one atomic operation and excluded some special situations. In reality, the AES execution can be interrupted. We did not consider the cases such as when the process scheduling and interruption occur during the AES. These conditions will be discussed later.%这里我是想说，在上面的这个证明里面，我们将AES过程当做了一个黑盒子，也就是一个原子操作。因此，我们有两种情况没有考虑，1是没有考虑中断或者调度发生在AES过程中，2是没有考虑周期调度函数执行，但并未发生调度。这两个情况将会在后面进行论证。在这里需要说明一下么？


\noindent\textbf{One AES execution is a partial warm operation.}
When the system is in the state $S_{as}$, one AES execution is a partial warm operation, as in this case, cache misses are resulted to load the corresponding entries into the cache.

%下面注释掉的这段是之前的证明，但是现在看来似乎是不需要了。
%Here, we prove that the extra time caused by not performing the \verb+warmup+ operation after an AES execution at the state $S_{as}$, is larger than our scheme. The probability that the next consecutive AES execution needs \verb+delay+ when $N$ cache line size of the lookup tables are in the cache before the AES execution (i.e.,  partial \verb+warmup+) is shown in Equation~\ref{equ:delay}, while the introduced extra time $T_{delay,N} = P_{delay,N} * t_{delay}$. The extra time $T_{warm,N}$ for our scheme is used to access all the lookup tables, including the ones in and not in the cache, and is shown in Equation~\ref{equ:partwarm}, where $t_c$ ($t_{nc}$) is the time for accessing one cache line that is in (not in) the cache. Evaluations on different platforms show that  $T_{delay,j} > T_{warm,j}$ for $j = 0, 1, ..., 67$.
%For example, in the environment described in Section~\ref{sec:eval}, $t_c = 5.01$, $t_{nc}=36.62$, $t_{delay}=4287$, the $T_{delay,j}$  and $T_{warm,j}$ are listed in Table~\ref{tbl:partwarm}.
%
%\begin{equation}
%\label{equ:partial}
%P_{delay,j} = \sum_{i=0}^{68-j}C_{68-j}^{i}(1-P_e)^{i}P_e^{68-j-i}(1-P_e^{68-i-j})
%\end{equation}
%\begin{equation}
%\label{equ:partwarm}
%T_{warm,j} = \sum_{i=0}^{68-j}C_{68-j}^{i}(1-P_e)^{i}P_e^{68-j-i}((i+j)t_c+(68-i-j)t_{nc})
%\end{equation}
%
%
%\begin{table}[!hbt]
%  \centering
%  \small
%  \caption{$T_{delay,j}$  and $T_{warm,j}$ (in ms).}\label{tbl:partwarm}
%   \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
%   \hline
%   {j}           & 0      & 1  & 2    & 10   & 20   & 30   & 40  & 50   & 60    & 67\\
%   \hline
%   {$T_{delay,j}$} & 4276.14 & 4275.14 & 4274.05 & 4260.84 & 4223.99 & 4135.21 & 3921.36 & 3406.20 & 2165.23 & 360.81 \\
%   \hline
%   {$T_{warm,j}$} & 535.11 & 532.18 & 529.26 & 505.60 & 474.86 & 440.87 & 398.26 & 332.83 & 206.97 & 34.42 \\
%   \hline
%   \end{tabular}
%\end{table}

%In our analysis, the warmup operation is performed after the AES execution. When doing the delay operation, the time consumption of the warmup operation is included in the delay time. So it is better to perform warmup when need to delay because the warmup cost nothing at this time.%这段话是想说，warm是在delay之后做的，而且warm的消耗包含在了delay的消耗里，所以只要需要进行delay的时候，进行一次warm是一个更好的选择，因为这个时候warm是不会产生额外消耗的。这段话放在这里似乎不太合适，但我没想好放在哪里。
When the system state is $S_{as}$, our scheme performs {\vwarm}  with probability 1, while the probabilistic  {\vwarm}  performs {\vwarm}   with the probability $P_{warm}$, which is increased by the AES execution, a partial warm operation. %, it mean the AES expands the $P_{warm}$.
However, according to Theorem~\ref{lemma:warmaftermiss},
$E(T^{p}) > E(T^{c})$ holds when  $1 \leq k \leq k_{0}$, which is independent of $P_{warm}$.
Therefore, our scheme is still better.


%the value of $P_{warm}$ doesn't affect the result. Although the AES expends the probability of warm, our scheme is better.
%When in the state $S_{as}$, for our conditional warmup method, we perform the warmup method. For probabilistic warmup method, originally performing the warmup operation is in a probability of $P_{warm}$. But now as the AES is a partial warmup operation, it mean the AES expands the $P_{warm}$. However, in the proof of Theorem 4, the value of $P_{warm}$ doesn't affect the result. Although the AES expends the probability of warm, our scheme is better.%这段话就是在解释AES本身是一个部分warm的过程。但是在现在的方案中，这个部分warm的效果，在我们的方案中没有体现，而在概率warm的情况下，效果就是提高了warm的概率。而我们在上面的证明过程中，得到了我们方案最优，是和概率warm的大小无关的，也就是说，这种情况下还是我们的方案更优。
%之前做的证明，是因为当时的方案是warm是在AES之前进行，所以warm执行不执行是会有时间开销的，所以是对warm开销和delay开销的一个比较。但现在，warm是没有开销的。所以把这段注释掉了。

\noindent\textbf{Process scheduling and interruption occur during the AES execution.}
In this case,  $end - start > T_{W}$, {\vwarm}  will be triggered in our scheme. %As proved in Theorem 2, we don't perform delay operation. So if we perform the warmup operation, it will bring extra overhead.
As the process scheduling and interruption occur, we assume that $N$ cache lines of lookup tables are evicted from caches.
%We assume that after this AES, there are $N$ cache line size of lookup tables not in the cache.
Hence, the extra overhead introduced by using {\vwarm}  to load all lookup tables is $E(T_{warm,N}) = N T_{cl} + (32-N)*t_{c}$, where $t_c$ and $T_{cl}$ denote the time for accessing one cache line from caches and RAM, respectively. If we do not perform \vwarm , the next round of AES may need to access part(s) of lookup tables from RAM instead of caches, which will trigger \vdelay. The expected overhead, denoted as $E(T_{delay})$, is shown in Equation~\ref{equ:tdelay}.
Table~\ref{tbl:scheduaes} shows that performing {\vwarm} achieves better performance.
%If not performing the warmup operation, the probability of needing delay is $P_{delay}$, and the extra time introduced is $T_{delay}$ as shown in Equation~\ref{equ:delay} and Equation~\ref{equ:tdelay}.
%So from Table~\ref{tbl:scheduaes}, we find that in this case, performing warmup operation is better.%这段是说明了调度和中断发生在AES执行过程中的情况。这时的warm是有额外开销的，比较这时使用warm的开销和不使用warm时会产生的delay的开销。得出此时使用warm是更好的。

\begin{table*}[t]
  \centering
  %\small
  %\scriptsize
  %\footnotesize
  \caption{The introduced time by performing warmup operation or not (in cycle).}\label{tbl:scheduaes}
   \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
   \hline
   {N}           & 1      & 2  & 3    & 5   & 10   & 15   & 20  & 25   & 30  &31  & 32\\
   \hline
   {$E(T_{delay})$} & 2509.29 & 2524.92 & 2524.99 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 \\
   \hline
   {$E(T_{warm})$} & 175.80 & 227.60 & 279.4 & 383.00 & 642.00 & 901.00 & 1160.00 & 1419.00 & 1678.00 & 1729.80 & 1781.60 \\
   \hline
   \end{tabular}
\end{table*}

\noindent\textbf{Periodical schedule function is called without scheduling during AES execution.}
As no other process invoked, the lookup tables are not evicted.
Therefore, {\vwarm}   is unnecessary although $T_{NM} < end - start \leq T_{W}$.
However, {\vwarm}   is a better choice, as we cannot predict whether the lookup tables are in the cache, and the overhead introduced by accessing the elements in caches is concealed by \vdelay.

%In this case, the lookup tables normally are still in the cache. So the warmup and delay operation are not needed. Actually, in this case, $T_{NM} < end - start \leq T_{W}$. However, we can't distinguish it is in this case or cache misses occurs. So we must do the delay operation. Moreover, in this case, performing warmup operation introduces none extra time. Therefore, we can regard this case as in the state $S_{as}$. %这段话是说明了周期调度函数调用但未有其他函数被调度时，AES时间是小于WET的，但是我们通过观察无法分辨是这种情况发生，还是是有cache miss发生。因此我们要对这种情况进行delay+warm的操作。其实对于概率warm的情况，也是要这么处理，所以，我们的方案合适。

\noindent\textbf{The relationship between the cache state and execution time.}
In Theorems 3 and 4, we prove that performing  {\vwarm}  when not all lookup tables are in caches, results the smallest extra time.
However, in our scheme, we perform the {\scshape{Warm}} operation according to the previous AES execution time,
 as we are technically unable to observe the cache state without introducing additional overhead.
% \footnote{Otherwise, we perform  the warmup operation before each execution, which is less efficient as proved in Theorem 4.}.% 这个脚注是什么意思？为什么？
The relation between the cache state and execution time is as follows:
\begin{itemize}
  \item $end - start \leq T_{NM}$, the system is in state $S_{full}$ or $S_{nonas}$, no {\vwarm}    is needed according to Theorem 4.
  \item $end - start > T_{W}$, the system is in state $S_{as}$ or $S_{nonas}$, {\vwarm}   is needed according to Theorem~\ref{theorm:warmall}.
 % another process is scheduled during the AES execution which makes part of the lookup tables  evicted from the cache.
  \item $T_{NM} < end - start \leq T_{W}$, %all the lookup tables may be in the cache or not, which is related with the workload in parallel.
%      The OS invokes the \verb+Scheduler()+ function every $t_{sched}$ interval and evicts at least one  entry of the lookup tables for anther process with the probability $P_{evict}$. Then, in this case,
%      when performing \verb+warmup+ for the AES execution next to the current one, the expected extra time is $T_{warm}^{t-w} = ( \frac{encEnd_c-encStart_c}{t_{interval}}*(1-P_{evict})+ \frac{encStart_c-encStart_p}{t_{interval}}*P_{evict}) * t_{warm}$;
%       while not performing the \verb+warmup+, the expected extra time is $T_{delay}^{t-w} = \frac{encStart_c-encStart_p}{t_{interval}}*P_{evict} * t_{delay}$. When $P_{evict}$ satisfies Equation~\ref{equ:con}, $T_{warm}^{t-w} < T_{delay}^{t-w}$, which means the \verb+warmup+ operation is not needed when the workload is small. In our evaluation environment, we find that when $P_{evict} < 0.18$, the \verb+warmup+ operation is unnecessary.
    %all the lookup tables may be in the cache or not.
    the system is in the state $S_{as}$ or $S_{full}$, {\vwarm}   is better according to previous analysis. %the periodic schedule function is called but scheduling not occurs during AES execution. But we treat them all as in the state $S_{as}$. On the above we already explain it reasonable.%这里就是说，这里对应两种情况，但是都只能当S_{as}这个状态来处理，但是之前已经说明过了，这样是没问题的。
\end{itemize}

%\begin{equation}
%\scriptsize
%\label{equ:con}
% P_{evict} < \frac{(encEnd_c-encStart_c)t_{warm}}{(encStart_c-encStart_p)t_{delay}+(encEnd_c-2encStart_c+encStart_p)t_{warm}}
%\end{equation}
%
%
%\begin{figure}
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=0.45\textwidth]{pic/statusandtime.pdf}\\
%  \caption{The consecutive AES execution.}\label{pic:status}
%\end{figure}


%Moreover, as proved in Lemma 3, the \verb+warmup+ operations  are necessary  to provide better performance in the following two  cases where  not all the lookup tables are in the cache:
%\begin{itemize}
%  \item Before the first AES execution, where none of the lookup tables are in  cache.
%  \item The interval between two AES executions is too large or perform AES discontinuously. %在AES不连续的时候，建议进行warm操作。
%\end{itemize}

\subsection{Different Key Lengths and Implementations}
%\vspace{-1mm}
\label{analy:otherkey}
%shall be discussed here!
%++++++++++
%Table~\ref{tbl:pdelayvalue} shows the minimum value of the $P_{delay}$ in different AES key lengthes and different AES implementations. The formulas of $P_{delay}$ is in the appendix.

\begin{table}[t]
%\vspace{-2mm}
  \centering
  \small
  \caption{the minimum value of the $P_{delay}$ in different cases.}
  \label{tbl:pdelayvalue}
   \begin{tabular}{|c|c|c|c|c|}
   \hline
   {table size}           & AES-128      & AES-192 & AES-256   \\
   \hline
   {5KB} & 0.85 & 0.88 & 0.90  \\
   \hline
   {4.25KB} & 0.907 & 0.944 & 0.966 \\
   \hline
   {4KB} & 0.924 & 0.954 & 0.973  \\
   \hline
   {2KB} & 0.994 & 0.998 & 0.999  \\
   \hline
   \end{tabular}
\end{table}

The {\scshape{Warm+Delay}} scheme provides the optimal performance for various implementations of AES~\cite{openssl,polarssl} with different key length, once $1 \leq k \leq k_0$.
All the proofs above stands, with the only difference be the value of $P_{delay}$ (detailed in Appendix~\ref{appendixb}).
$P_{delay}$ depends  on the size of lookup tables and the number of iterated rounds.
For mbed TLS-1.3.10~\cite{polarssl} and OpenSSL-0.9.7i~\cite{openssl}, the size of lookup tables are 4.25KB and 5KB. For OpenSSL-1.0.2c~\cite{openssl}, the size of lookup tables is 4KB or 2KB. The number of rounds is 10, 12 and 14 for AES-128, AES-192 and AES-256,respectively.
Table~\ref{tbl:pdelayvalue} lists the corresponding minimum $P_{delay}$.


%We analyze three different AES implementations. The difference is in the last round of the AES. The first one uses a 256B table in the last round~\cite{polarssl}. The second one uses one 1KB table in the last round~\cite{openssl}(Openssl-0.9.7i), and the third one uses the four tables in the previous rounds~\cite{openssl}(Openssl-1.0.2c). Also we analyze the different key length of AES. AES-128 has 10 rounds, AES-192 has 12 round, and AES-256 has 14 rounds.%这是说，有三种AES实现，一种是使用5个查找表，前面几轮使用4个1K的表，最后一轮使用256B的一个表。一种是使用5个表，前几轮也是使用4个1K的表，但最后一轮使用1个1K的表。还有一种是使用4个1K的表，每轮都是使用这4个表。同时每种实现有分为三种密钥长度，每种密钥长度其实对应的是AES不同的轮数.

%For all the situations, the process of the proof of the Theorem 3 and Theorem 4 is almost the same. The main difference is the value of $P_{delay}$. We compute the value of $P_{delay}$ in the Table~\ref{tbl:pdelayvalue}, and find out that the Theorem 3 and Theorem 4 still hold true. %这段是说，对于这些不同的情况，在证明Lemma 3 and Lemma 4的时候，证明方法都是一样的，不同的就是P_{delay}的取值范围不同，但是经过计算，在每种取值范围内，Lemma 3 and Lemma 4都是成立的，也就是都是我们的方案最优。

For other table-lookup block ciphers, we have to determine the $P_{delay}$ according to the algorithm and the implementation.
Once $1 \leq k \leq k_0$ is satisfied , {\scshape{Warm+Delay}}  provides the optimal performance.

%it is similar to the AES. They just have different values of $P_{delay}$. The three states of the cache is same and they have the same state transition diagram. So it can be proved that the Theorem 3 and Theorem 4 hold true for them.% 这段是说，对于其他算法，Lemma 3 and Lemma 4 的证明也是类似的，同样对于cache有这三种状态，且状态转移图也是一样的，故而证明过程也一样，只是P_{delay}不同而已，可以证明对于其P_{delay}的范围，Lemma 3 and Lemma 4 也是成立的。



