\section{Towards the Optimal Performance of {{\scshape{Warm+Delay}}}}
\label{sec:performanceproof}
%\vspace{-1mm}
\def \vdelay {{\scshape{Delay}}}
\def \vwarm {{\scshape{Warm}}}
\def \vwd {{\scshape{Warm+Delay}}}

%In this section, we first introduce five principles that lead to an optimized {\scshape{Warm+Delay}} performance. We then examine these principles with our proposed algorithm, and derive practical conditions which make the proposed algorithm optimal. We first examine the performance of \vdelay, and then analyze the \vwam~operation with AES as the cryptographic algorithm, and show that our scheme is optimal. Last, we discuss the impact of different key lengths and different algorithm implementations.
%All the detailed conditions are practical.


In this section, we apply the {\scshape{Warm+Delay}} scheme in Algorithm~\ref{alg:aesdefense}  to AES.
We first discuss the factors for the integration scheme to produce the optimized long-term performance.
Then we present the conditions for the {\scshape{Delay}} operations to get the optimal performance.
 Meanwhile we use a statistical model to examine the {\scshape{Warm}} operations.
Finally, we propose a strategy for the {\scshape{Warm+Delay}} scheme,
   and quantitatively prove that the optimal performance is achieved through our strategy.

\subsection{Overview}
\label{sec:eval}

In the {\scshape{Warm+Delay}} scheme,
        the extra operations are introduced by {\scshape{Warm}}, {\scshape{Delay}} and {\scshape{GetTime}}.
   Because the cost of {\scshape{GetTime}} is static and always necessary,
 the additional overhead is determined by {\scshape{Warm}} and {\scshape{Delay}}.
Therefore, to achieve the optimal performance, we shall discuss the following questions:

\begin{enumerate}[(1)]
 \item Between {\scshape{Warm}} and {\scshape{Delay}}, which is the preferred operation?

 \item What is the best strategy for the {\scshape{Delay}} operations that will produce the optimal performance in the long term?
 The strategy includes two aspects: (2.A) When shall {\scshape{Delay}} be imposed? and (2.B) What is the optimal delay to be imposed?

% When the {\scshape{Delay}} operation is performed, what is the best strategy that would result the best performance while ensuring security? In particular,

 \item What is the best strategy for the {\scshape{Warm}} operations? It also includes two aspects: (3.A) When shall {\scshape{Warm}} be imposed? and (3.B) What is the optimal amount of lookup table entries to be accessed in {\scshape{Warm}}?

% When the {\scshape{Warm}} operation is performed, what is the best strategy that would result the best performance? In particular, what is the optimal amount of data to be loaded during {\scshape{Warm}}?
\end{enumerate}
%\begin{itemize}
%    \item In the {\vdelay} operation, the imposed delay, or the execution time of padding instructions, is the shortest.
%    \item The number of {\vdelay} operations is minimum.
%    \item In the {\vwarm} operation, only the minimum necessary data are loaded into caches. % as little as possible.
%    \item The number of  {\vwarm} operations is minimum.
%    \item Because {\vwarm} is much more efficient than {\vdelay},{\footnote{It is true for any computing platform in practice; otherwise, caches are useless in performance improvement.}} {\vwarm} is preferred over {\vdelay}. % than \verb+delay+.
%\end{itemize}

When attempting to answer these questions,
 we focus on the optimization of the long-term performance.
 That is, our objective is to optimize the overall performance of encrypting large amounts of data,
  instead of the performance of encrypting one block (i.e., short-term performance).
%In this section, we will provide a comprehensive quantitative analysis on how to achieve this long-term optimal.

%In particular, we may sacrifice .... if the additional (seemingly unnecessary) operations in this encryption will improve the encryption speed in the future.


The first question is relatively easy to answer. The {\vwarm} operation, which loads lookup tables into caches, will speed up the following encryption.
Meanwhile, the {\vdelay} operation, which inserts padding instructions to eliminate timing side channels,
 increases the overhead.
    Therefore, intuitively, the {\scshape{Warm}} operation is %always
     preferred over {\vdelay}. %in the {\scshape{Warm+Delay}} scheme.

In our analysis (as in other implementations),
 an instruction loop imposes {\scshape{Delay}} which accesses no memory,
 %\footnote{We discuss the {\scshape{Warm}} operations, performed as a part of {\scshape{Delay}}, in Section \ref{remark}}
 so the cache state is not changed by {\scshape{Delay}}.
The {\scshape{Delay}} operation only changes the execution time of the current encryption,
  while it will \emph{not} affect the following executions.
So the answers to Question (2) are: (2.A) We should only impose {\vdelay} when it is required for security purposes (i.e., to impose the minimal number of {\vdelay} operations); and (2.B) The length of the imposed delay should be just enough to ensure security, but not more than that (i.e., to impose the minimal padding  in each {\vdelay} operation).

Meanwhile,
 the {\scshape{Warm}} operation not only changes the execution time of the current encryption,
  but also has a lasting effect on the next one(s).
  Intuitively, loading less data during one {\scshape{Warm}} operation introduces a smaller overhead for the current encryption;
 however, it may result in more {\vdelay} and {\vwarm} operations in the future.
 In this case, optimized short-term performance may cause worse long-term performance, which is not desirable. Therefore, to answer Question (3),
  we need to build a quantitative model for the impacts of a partial/full {\vwarm} on future encryption.
  It is the main challenge of this work.


%   It is because the amount of data loaded in {\scshape{Warm}} operation and when performing it may lead to the extra {\scshape{Delay}} operation.
%    So the optimal amount of data loaded in {\scshape{Warm}} operation and the optimal execution occasion should be found in order to achieve the optimal performance.


In the next, we first examine the optimal conditions for {\scshape{Delay}}.
 We prove that,
  it provides the optimal performance while ensuring security
  if delaying to $T_{mm}$ in {\scshape{Delay}} and performing {\scshape{Delay}} only when the encryption time is in $(T_{nm} ,T_{mm})$.
    Then we investigate the {\vwarm} operation for AES-128 with 2KB lookup tables~\cite{openssl}.
     We prove that it is statistically the most efficient to load all lookup table entries in {\vwarm} and perform it
      if a cache miss occurs during the previous encryption.
Finally, we extend the conclusions to different key lengths of AES and different implementations,
  and show that, the conlusions are applicable to these typical key lengths and implementations.
That is, the derived {\scshape{Warm+Delay}} scheme achieves the optimal performance for various AES implementations with lookup tables.

%Next, we examine these principles with the {\vwd} scheme, and derive the practical conditions which make it optimal.
%We first examine the performance of \vdelay, and then analyze the {\vwarm} operation for AES-128 with 2KB lookup tables \cite{openssl}.
%In the proof, we show that it is statistically the most efficient to load all lookup tables in \vwarm, instead of loading parts of the tables (which probably leads to more {\vdelay} in the future).
%Then we identify the best condition for the {\vwarm} operation,
%  and eventually derive the practical condition so that the {\vwd} scheme (with appropriate parameters) is optimal.
%
%Finally, we extend the conclusions to different key lengths of AES and different implementations,
%  and show that, the conditions are applicable to these typical key lengths and implementations.
%That is, the derived {\scshape{Warm+Delay}} scheme achieves the optimal performance for various AES implementations with lookup tables.

%In our {\vwd}  scheme, we perform delay operation when the execution time is in $(T_{nm}, T_{mm})$, and delay it to $T_{mm}$. The {\vwarm}  (cache warm) operation in our scheme loads all the lookup tables into the cache. Hence, the overhead depends on the size of the lookup tables and the implementation of algorithms. We implement our scheme in AES-128 with 4.25KB lookup tables in the proof in Section 4.3. In the proof, we show that it is statistically the most efficient to load all lookup tables in \vwarm, instead of loading parts of the tables (which leads to more {\vwarm}  in the future). Then we identify the best timing for the {\vwarm}  operation. In particular, we will discuss three cache-access states: (1) all the lookup tables are in the cache ($S_{full}$); (2) the lookup entries not in the cache are not accessed ($S_{nonas}$); (3) and the entries not in the cache are accessed ($S_{as}$). We will compare the additional time introduced by our scheme vs. other methods. Eventually, we derive a practical condition, so that our scheme is optimal when the condition is satisfied. That is, performing warmup operation when the execution time lies in $(T_{nm}, \infty)$ is the optimal method in our experiment environment.

%The sketch is needed here!+++++++++++

%Next we discuss \verb+delay+ and \verb+warmup+ in details. we discuss the \verb+warmup+ operation after the \verb+delay+ operation because the \verb+warmup+ has some relation to the \verb+delay+.

%\begin{CJK}{UTF8}{gkai}
%最优化证明的目的在于，证明我们的方案是最优的，而保证最优的原则有以下几点：
%1、	warm内容尽可能少
%2、	warm的次数尽可能少
%3、	delay的时间尽可能短
%4、	delay的次数尽可能少
%5、 原则上由于delay时间大于warm时间，因此尽可能多的使用warm，和尽可能少的使用delay
%然后对warm 和delay 分别进行详细证明。
%\end{CJK}

\subsection{The Optimal Conditions for {\vdelay}}
\label{delayanalysis}
%\vspace{-2mm}
%The analysis of {\vdelay}  is independent of the encryption algorithm used in the scheme.
\begin{theorem}
\label{delay2tw}
To effectively eliminate the cache timing side channels,
 in the {\vdelay} operation delaying to $T_{mm}$ imposes the minimal delay.
%In the {\vdelay} operation, delay to $T_{mm}$ imposes the minimal overhead
 %  while effectively eliminates the cache timing side channels.
\end{theorem}

\begin{proof}
From the attackers' point of view,
   $T_{nm}$, $T_{mm}$ and any value greater than $T_{mm}$ are three types of measured time that are unexploitable;
   i.e. the measured time does not reflect the cache misses/hits of certain lookup table entries.
    %$T_{nm}$, $T_{mm}$, and any value greater than $T_{mm}$.
%$T_{nm}$  means that \emph{all} accessed lookup entries are in caches before encryption,
 %and
%$T_{mm}$ corresponds to the longest execution path, when all lookup tables are not cached before encryption (and they are loaded into caches as the encryption operation is performed);
 %so such results are the same for any input.
%When the measured time is longer than $T_{mm}$,
  %encryption is disturbed by unknown system activities (e.g. interrupts and task scheduling),
  % and it is impossible for the attackers to exclude the disturbance and find the exact execution time of encryption.

If we delay the execution time to any value greater than $T_{mm}$,
 though the security is guaranteed,
the overhead is larger than that delays to $T_{mm}$.
On the other hand, if we delay the time to any value less than $T_{mm}$,
 a little information about the cache access leaks.
Meanwhile, if we delay it to a random/pseudorandom value, which may be greater or less than $T_{mm}$,
    the attackers could exclude all results greater than $T_{mm}$
      and the other results are still exploitable.
Such methods reduce the attack accuracy,
  but do not eliminate the timing side channels completely.
Therefore, delaying to $T_{mm}$ imposes the minimal overhead
   while effectively eliminates the cache timing side channels.
%\qed
\end{proof}


\begin{theorem}
\label{delaynum}
 Performing the {\vdelay} operation if and only if the execution time of encryption
 is in $(T_{nm}, T_{mm})$,
 results in the least number of {\vdelay} operations. % when we cannot distinguish the reason why encryption takes longer than $T_{nm}$ (cache miss or the OS scheduler and  interrupt handler).
\end{theorem} % 这里when后的部分需要吗
%******BL: 我也觉得后面半句可以不要，因为下文有解释。我先把这半句注释掉了


\begin{proof}
As described above,
 if the encryption execution time is equal to or less than $T_{nm}$, no cache miss occurs,
  and
if the time is equal to $T_{mm}$ or greater, it is unexploitable.
In these cases, the execution time is independent of keys and plaintexts/ciphertexts,
  so {\vdelay} is unnecessary.


%If the encryption time is greater than $T_{mm}$,
% it means that the OS scheduler or interruption handler has been invoked during encryption -- the attacker could not recover the actual encryption time to perform timing attacks, because the time for OS scheduler or interruption handler is unpredictable.

When the execution time is in $(T_{nm}, T_{mm})$,
 it may result from cache misses due to the access of the uncached table entries or system activities (but the overhead is less than $T_{mm} - T_{nm}$).
We cannot distinguish the exact reasons without special privileges.
However, the attackers may eliminate the effect of system activities by repeatedly invoking the encryption function using the same input.
 Therefore, {\vdelay} is necessary in this case.
%\qed
\end{proof}

It is easy to verify that, in the scheme in Algorithm~\ref{alg:aesdefense},
 the {\vdelay} operation satisfies Theorems~\ref{delay2tw} and \ref{delaynum}.
% So the use of {\vdelay} can achieve the optimal performance in the {\vwd}  scheme.


\subsection{The Optimal Conditions for {\vwarm}}
\label{warmanalysis}
%\vspace{-1mm}

We apply the {\vwd}  scheme to AES-128 with a 2KB lookup table \cite{openssl},
  as described in Section \ref{back:aes}.
%The {\vwarm}  operation is applied to the lookup tables, which consume 2KB in total.
We first determine the amount of data to be loaded in {\vwarm} for the optimal performance.
 Then, in order to explore the best conditions of performing the {\vwarm} operation,
  we use Markov Chain to model different {\vwarm} strategies
  and compare their performance.
We show that, %in this case,
    the best conditions of performing the {\vwarm} operation (i.e., with the optimal performance)
 hold in commodity computer systems.
%In Section \ref{analy:otherkey}, we also discuss other key lengths and other implementations and other algorithms.

%\vspace{2mm}
%\noindent\textbf{1. The Amount of Data To Be Loaded in {\vwarm} Operation.}


\subsubsection{The Amount of Data Loaded in {\vwarm}}
A {\vwarm} operation loads the lookup tables into caches.
 It takes less time if only a part of the table is loaded,
  but it may introduce extra {\vdelay} operations if the unloaded entries are accessed.
   The benefit of not loading $N$ cache lines of data in the {\vwarm} operation is denoted as $B_{nl}(N)$,
    while the expected overhead due to not loading $N$ cache lines is denoted as $D_{nl}(N)$.
So the amount of data to be loaded in {\vwarm} is determined by comparing $B_{nl}(N)$ with $D_{nl}(N)$.

We denote the size of a cache line as $C$,
    and the time cost to load a cache line of data from RAM to L1D caches as $T_{cl}$.
We have the following theorem for an AES implementation of $R$ rounds with an $L$-byte lookup table ($L \gg C$).

%The average time saved by not loading one cache line from RAM is denoted as $t_{nc}$,


%\begin{theorem}\label{theorm:warmall} For the hardware of commodity computers, loading all the AES lookup tables into the cache is the best warm operation.\end {theorem}
%\begin{theorem}\label{theorm:warmall}
%If $B_{nl}(N) < D_{nl}(N)$ for any $0 < N \leq \frac{L}{C}$,
% loading all entries of the lookup table into caches in the {\vwarm} operation provides better performance
% than loading any part of entries,
%where $B_{nl}(N) = N T_{cl}$ and $D_{nl}(N) = (1- (1-\frac{NC}{L})^{16 R})(T_{mm}-T_{nm})$.
%\end{theorem}
\begin{theorem}\label{theorm:warmall}
If $B_{nl}(N) < D_{nl}(N)$ for any $0 < N \leq \frac{L}{C}$,
 loading all entries of the lookup table into caches in the {\vwarm} operation provides better performance
 than loading any part of entries,
where $B_{nl}(N) = N T_{cl}$ and $D_{nl}(N) = (1- (1-\frac{NC}{L})^{16 R})(T_{mm}-T_{nm})$.
\end{theorem}

\begin{proof}
%It takes $T_{cl} > 0$ to load data into a cache line in the {\vwarm} operation;
% while,
% if some entry is uncached,
%  the execution time of AES encryption may become longer and then it triggers an extra {\vdelay} operation.
Not loading $N$ cache lines of table entries saves the execution time of \vwarm,
 while the potential cost is
    the longer execution of encryption  and the extra execution of \vdelay.
Since $L \gg C$, we needs $\frac{L}{C}$ cache lines to hold the lookup table.
In each round of AES encryption,
 the lookup table is accessed for $16$ times.
We assume that, in each round, the input of \texttt{SubBytes} is random and then
    each table entry is accessed  uniformly.
Hence,
 the probability that $N$ certain cache lines are not accessed after $R$ rounds of AES encryption, denoted as $P_{\bar{a}}(N)$,
  is $P_{\bar{a}}(N) = (1-\frac{NC}{L})^{16 R}$.



%In AES, {\vwarm}  loads all lookup tables into cache. In the case that at least one corresponding cache line is \emph{not} loaded,
% cache miss may happen, which will in turn trigger {\vdelay}  operation. Therefore,

%For {\vwarm} operation, if there exists $N>0$ making the time saved by not warming $N$ cache lines larger than the expected overhead of invoking {\vdelay}, not warming $N$ cache lines provides better performance. Otherwise, loading all lookup tables into the cache is the best.


%For AES with 2KB lookup table implementation, it just use one lookup table $L_{2k}$ with 2KB.
%For a cache line utilized by AES look up tables, the probability that it loads $T_{0}, T_{1}, T_{2}, T_{3}$ is $16/17$, while the probability it loads the content of $T_{4}$ is $1/17$.

%Therefore, the probability that a certain cache line of $L_{2k}$ is not accessed in all rounds is $P_{1} = (1-\frac{C}{L})^{160}$.
%The expected probability that any cache line is not accessed in one AES execution is $P_{e} = \frac{16}{17}P_{0-3}+ \frac{1}{17}P_{4}$.

If a table entry is uncached but accessed during encryption,
   the execution time is greater than $T_{nm}$ and {\vdelay} is performed.
Therefore, if $N$ cache lines of table entries are not in caches,
    the probability of extra {\vdelay} is:
  \begin{equation}
  \label{equ:delay}
P_d(N) = 1-P_{\bar{a}}(N) = 1- (1-\frac{NC}{L})^{16 R}
\end{equation}


The execution time shall be delayed to $T_{mm}$ according to Theorem \ref{delay2tw},
 while it is $T_{nm}$ if all entries are in caches.
So, %the expected overhead (or the time cost) due to not loading $N$ cache lines of table entries is:
\begin{eqnarray}
 \label{equ:tdelay}
D_{nl}(N)%&={ }  {P}_{d}(N)  (T_{mm}-T_{nm})  \nonumber \\
= (1- (1-\frac{NC}{L})^{16 R})(T_{mm}-T_{nm})
\end{eqnarray}

The time cost to load a cache line of data is $T_{cl}$, so $B_{nl}(N) = N T_{cl}$.
%the benefit  of not loading data into $N$ cache lines in the {\vwarm} operation is
Therefore, we conclude that
(\emph{a})
if $B_{nl}(N) < D_{nl}(N)$ for any $\frac{L}{C} \geq N > 0$,
 loading all entries of the lookup table into caches in the {\vwarm} operation provides better performance;
% (1) If $\forall N>0, \Delta T_{warm} < E(T_{delay})$, warming all cache lines is always better.
and (2) if there exists $N$ satisfying that $B_{nl}(N) > D_{nl}(N)$, not loading $N$ cache lines of table entries does better.
%  and (3) If $\exists N>0, \Delta T_{warm} = E(T_{delay})$, warming or not warming $N$ cache lines provides equivalent performance.
%\qed
\end{proof}


%In the {\scshape{Warm+Delay}} scheme, {\vwarm} is performed  after encryption
%    as a part of the {\vdelay} operation, as shown in Algorithm \ref{alg:aesdefense}.
%So the \emph{actual} cost of loading data into $N$ cache lines (or
%   the benefit of not loading these data) in {\vwarm} is less than $B_{nl}(N)$.
%For example,
%    when the time of {\scshape{Crypt}} plus {\vwarm} is less than  $T_W$ (Line 8 in Algorithm \ref{alg:aesdefense}),
%  the  cost of {\vwarm} is actually zero.
%
%Therefore,
%  the average cost of loading data into $N$ cache lines in the {\vwarm} operation is \emph{actually} less than $N T_{cl}$,
%      so loading all entries in {\vwarm} still provides better performance
% than any part of entries if $B_{nl}(N) = N T_{cl} < D_{nl}(N)$ for any $\frac{L}{C} \geq N > 0$.


% Therefore, the time of {\vwarm}  becomes part of delay.
% When this case occurs, the extra time reduced by warming some cache line is zero,
% if the AES execution time plus {\vwarm}  time is less than $T_{mm}$.


% 这一段我是想说，咱们的方案中，warm 是在AES 计算之后进行的，当AES计算如果大于WET的时候，warm之后不进行delay，所以warm少几个cache line节省的开销是按照上面的计算，说明warm 比不warm 要优。而在AES计算小于WET 时，warm操作本身就是delay的一部分，那么可以认为warm的操作开销减小，这时warm 自然是全部warm 最优。

\begin{corollary}
\label{corollarywarm1}
For the AES-128 implementation with a 2KB lookup table
in commodity computer systems,
loading all entries of the lookup table into caches in the {\vwarm} operation
   provides better performance than loading any part of entries.
\end{corollary}

\begin{proof}
With commodity hardware, the cache is enough for all entries of the lookup tables.
For example, AES is implemented with the lookup tables of 2KB, 4KB, 4.25KB or 5KB,
   and the lookup tables of DES/3DES are 2KB.
Meanwhile, the typical L1D cache is 32KB or 64KB for Intel CPUs, and 16KB or 32KB for ARM CPUs.


In our experiments on a Lenovo ThinkCentre M8400t PC with an Intel Core i7-2600 CPU and 2GB RAM,
$T_{mm}$, $T_{nm}$, and $T_{cl}$ are $2834.00$, $355.00$, and $54.55$ CPU cycles, respectively
 (see Section \ref{sec:implementation} for details).
%%% 计数的数字保留，最好一致。例如，都是小数点后1位、2位，固定一致。
Table~\ref{tbl:warmcontent} shows $B_{nl}(N)$ and $D_{nl}(N)$,
    when $R = 10$, $L = 2$KB, and $C = 64$B.
We find that, the cost of not loading $N$ ($32 \geq N >0$) cache lines of the lookup table is always much greater than the benefit.
\textcolor[rgb]{1.00,0.00,0.00}{The result is applicable to other commodity computer systems.}
Then, according to Theorem \ref{theorm:warmall}, loading all entries in {\vwarm} produces better performance.
\end{proof}

Moreover, in our {\vwd} scheme, {\vwarm} is performed % after encryption
   as a part of the {\vdelay} operation, as shown in Algorithm \ref{alg:aesdefense}.
So the cost of {\vwarm} is further reduced, ensuring that loading all entries of the lookup table in {\vwarm} is better.

 %The time $T_{mm}$ is 2834 CPU cycles, while the shortest AES execution time $T_{nm}$ is 309 cycles. The average time $T_{cl}$ for loading one cache line of lookup table is 55.68 cycles.

\begin{table*}[t]
  \renewcommand{\arraystretch}{1.3}
  \caption{Benefit and expected cost of not loading $N$ cache lines of table entries (in CPU cycles).}\label{tbl:warmcontent}
  \centering
   \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
   \hline
   {$N$}           & 1      & 2  & 3    & 5   & 10   & 15   & 20  & 25   & 30  & 31  & 32\\
   \hline
   {$B_{nl}(N)$} & 54.55 & 109.10 & 163.65 & 272.75 & 545.50 & 818.25 & 1091.00 & 1386.25 & 1636.50 & 1691.05 & 1745.60 \\
   \hline
   %{$D_{nl}(N)$} & 2509.29 & 2524.92 & 2524.99 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 \\
   {$D_{nl}(N)$} & 2463.58 & 2478.92 & 2478.99 & 2479.00 & 2479.00 & 2479.00 & 2479.00 & 2479.00 & 2479.00 & 2479.00 & 2479.00 \\
   \hline
   \end{tabular}
\end{table*}




%************************************分隔符******************************************
%\begin{theorem}\label{lemma:warmaftermiss}
%For commodity computers, performing  {\vwarm}  when cache-miss occurred during the previous execution results the minimum extra time.
%\end{theorem}
%\vspace{2mm}
%\noindent\textbf{2. State Transition of Warm Strategies.}

\subsubsection{State Transitions of Different {\vwarm} Strategies}
We use Markov Chain to analyze the states of the lookup table in caches during the continuous invocations of AES encryption.
Then the extra costs are calculated based on the probability of each state in the stationary distributions,
    to determine the optimal {\vwarm} strategy.
There are three states after one AES execution (i.e. {\scshape{Encrypt}} in Algorithm~\ref{alg:aesdefense}):
  \begin{enumerate}
   \item
     $S_{c}$: all entries of the lookup table are in caches,
   \item
     $S_{\bar{c}, a}$: some entries are uncached, and at least one of them is accessed during the AES encryption,
   \item
     $S_{\bar{c}, \bar{a}}$: some entries are uncached, but none of them is accessed during the AES encryption.
  \end{enumerate}
$S_{\bar{c}, a}$ and $S_{\bar{c}, \bar{a}}$ represent the states where one or more cache lines of table entries are uncached or evicted from caches.

The state is estimated after each execution of AES encryption.
The transition among three states is triggered by three events:
\begin{enumerate}
  \item
    {\scshape{Warm}}: Performing {\vwarm} to load all table entries into caches, and the probability is $P_{w}$.
  \item
    {\scshape{Eviction}}: Some table entries of may be evicted from caches when task scheduling, interrupts or other system activities occur during the execution. The probability that such an event occurs is denoted as $P_{e}$.
  \item
    {\scshape{Access-Uncached}}: For one execution of AES encryption,
     some entries are uncached before encryption and it is possible that some of them is accessed during encryption,
      and this probability is denoted as $P_{a}$. Actually, $P_{a}$ is equal to $P_{d}$ in Theorem~\ref{theorm:warmall};
      i.e., {\scshape{Access-Uncached}} results in {\vdelay}.
\end{enumerate}

% 哪些事件导致状态转移？概率表示为。
% 事件的定义
%warm:
%evict:
%access: P   $\bar{P}$

%$P_{nona}$ (equals to $P_{N}$) is the probability that the entries not in the cache are not needed in one execution; while $P_{evict}$ is the probability that some entries are evicted from the cache.


%在状态转移过程中，cache状态发生转移，则需执行一次AES。
%上述三种事件的发生，会使得状态发生改变。
%假设三种事件在状态转移过程中只发生一次。
%AES执行肯定是最后发生，因为执行结束，cache状态以发生转移。
%先warm后evict

%These events change the states of the lookup table.
 %During the state transition, the state of lookup tables changes which means one AES executes.
Without loss of generality,
we assume that during one state transition, each of the events occurs once at most.\footnote{Even if some occurs more than once actually,
    they can be viewed as one event with  the strengthened impact on the state transition.}
  % Because the state changes after the \emph{Execution} event, the \emph{Execution} event occurs the last.
We assume that,
    during one state transition,
    {\scshape{Warm}} occurs before {\scshape{Eviction}} if both of them occur.
It is reasonable,
    because
    an {\scshape{Eviction}} event occurring before  {\scshape{Warm}} has no effect on the state of the lookup table,
        and then can be ignored.
Since the state of the lookup table is observed immediately after the AES execution,
  {\scshape{Access-Uncached}} occurs in the last during one state transition (if occurs).
In the following analysis, we firstly ignore the partial-warm effect of {\scshape{Access-Uncached}} in the analysis of the different strategies,
    and in Section~\ref{remark} we show that
    this effect has no influence on our conclusions.


We consider three strategies:
%In {\vwd} scheme, the warm strategy that performing {\vwarm} when some cache misses occur during the pervious execution of encryption is denoted as conditional {\vwarm}.
%While considering other two cases:
\begin{itemize}
  \item
    Conditional {\vwarm}: performing {\vwarm} when some cache misses occur during the pervious encryption execution, as done in the {\vwd} scheme;
  \item
    Less {\vwarm}: performing {\vwarm} with a probability $P_{w}^{\ominus}\in[0,1)$  when cache misses occur during the pervious encryption execution;
  \item
    More {\vwarm}: performing {\vwarm} when some cache misses occur during the pervious encryption execution, and also performing with a probability $P_{w}^{\oplus}\in(0,1]$ in other conditions.
\end{itemize}

Markov Chain is used to simulate the state transition.
 The state of the lookup table denoted as $X_{n}$,
     takes values in the state space $\{S_{c},S_{\bar{c}, a},S_{\bar{c}, \bar{a}}\}$.
  %The transition probabilities denoted as $P_{j|i}$, where $i$ and $j$ are in the state space, represents that
   If it is in the state $i$, the probability that it will be in the state $j$ after one step is denoted as $P_{j|i} = P\{X_{n+1}=j|X_n=i\}$, where $i,j \in \{S_{c},S_{\bar{c}, a},S_{\bar{c}, \bar{a}}\}$.
Also, $P_{a|i}$ and  $P_{e|i}$ 
    represent the probabilities that {\scshape{Access-Uncached}} and {\scshape{Eviction}}
     occur %(due to the occurrence of the {\scshape{Eviction}} event)
    in the state $i$, respectively, where $i \in \{S_{c},S_{\bar{c}, a},S_{\bar{c}, \bar{a}}\}$.
% $P_{e|i}$ is to represent the probability of the occurrence of the event in the state $i$, where $i \in \{S_{c},S_{\bar{c}, a},S_{\bar{c}, \bar{a}}\}$.
Besides,
    we use $\ominus$ and $\oplus$ to distinguish the symbols of less and more {\vwarm} as the superscripts, respectively.



Next,
the stationary distributions for the three different strategies are analyzed as follow.

\begin{figure*}[t]
    \centering
    \subfloat[Conditional warm]{\includegraphics[width=2.9in]{pic/conditionwarm2.pdf}
    \label{fig:conditioncase}}
    \hfil
    \subfloat[Less warm]{\includegraphics[width=3.15in]{pic/probwarmless2.pdf}
    \label{fig:lesscase}}
    \hfil
    \subfloat[More warm]{\includegraphics[width=3.5in]{pic/probwarmmore2.pdf}
    \label{fig:morecase}}
    \hfil
    \subfloat[Unusual warm]{\includegraphics[width=3.5in]{pic/probwarmco2.pdf}
    \label{fig:cocase}}
    \caption{The Markov state transition probability diagram.}
    \label{pic:statetrans}
\end{figure*}

\vspace{2mm}
\noindent\textbf{Conditional {\vwarm}.}
The state transition of conditional {\vwarm} is shown in \figurename~\ref{fig:conditioncase}.
  %Due to the difference of the quantity of entries of the lookup table not in the caches, $P_{a|S_{\bar{c}, \bar{a}}}$ is no less than $P_{a|S_{c}}$.
In the state $S_{c}$, {\scshape{Warm}} is not performed.
If {\scshape{Eviction}} does not occur, {\scshape{Access-Uncached}} does not occur and the state remains, i.e. $P_{S_{c}|S_{c}}=1-P_{e|S_{c}}$.
When {\scshape{Eviction}} occurs, some table entries are evicted from caches.
Then if these evicted entries are accessed during the AES execution,
 it turns to $S_{\bar{c}, a}$ and $P_{S_{\bar{c}, a}|S_{c}}=P_{e|S_{c}}P_{a|S_{c}}$;
otherwise, it turns to $S_{\bar{c}, \bar{a}}$ and $P_{S_{\bar{c}, \bar{a}}|S_{c}}=P_{e|S_{c}}(1-P_{a|S_{c}})$.

++++++

In the state $S_{\bar{c}, a}$, {\scshape{Warm}} is performed to load all entries into caches.
Then the state transition is similar to that in $S_{c}$.
If {\scshape{Eviction}} does not occur, it turns to $S_{c}$ and $P_{S_{c}|S_{\bar{c}, a}}=1-P_{e|S_{c}}$.
When {\scshape{Eviction}} occurs, and if the evicted entries are accessed during encryption,
 the state remains the same, thus $P_{S_{\bar{c}, a}|S_{\bar{c}, a}}=P_{e|S_{c}}P_{a|S_{c}}$.
Otherwise it turns to the state $S_{\bar{c}, \bar{a}}$ and $P_{S_{\bar{c}, \bar{a}}|S_{\bar{c}, a}}=P_{e|S_{c}}(1-P_{a|S_{c}})$.

    When in the state $S_{\bar{c}, \bar{a}}$, the {\scshape{Warm}} is not performed because the cache misses do not occur. Whether the {\scshape{Eviction}} event occurs or not, some lookup entries are not in the cache. So if these uncached entries are accessed during the AES execution, it turns to the state $S_{\bar{c}, a}$. If the {\scshape{Eviction}} event occurs, the probability is $P_{e|S_{\bar{c}, \bar{a}}}P_{a|S_{\bar{c}, \bar{a}}}$. And if not occur, the probability is $(1-P_{e|S_{\bar{c}, \bar{a}}})P_{a|S_{c}}$.
     Thus, the probability to the state $S_{\bar{c}, a}$ is $P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}}}=P_{e|S_{\bar{c}, \bar{a}}}P_{a|S_{\bar{c}, \bar{a}}}+(1-P_{e|S_{\bar{c}, \bar{a}}})P_{a|S_{c}}$.
    Otherwise the uncached entries are not accessed, it remains the state $S_{\bar{c}, \bar{a}}$ and $P_{S_{\bar{c}, \bar{a}}|S_{\bar{c}, \bar{a}}}=1-P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}}}$.


We use $\Pi_{c}$, $\Pi_{\bar{c}, \bar{a}}$, $\Pi_{\bar{c}, a}$ to represent
the stationary distribution of the three states and the values are as follows.
%\setlength{\arraycolsep}{0.0em}
\begin{equation}
\label{equ:pic}
\left\{
\begin{split}
&\Pi_{c} = \frac{P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}}(1-P_{e|S_{c}})}{P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}}}+P_{e|S_{c}}(1-P_{a|S_{c}})}\ \\
&\Pi_{\bar{c}, \bar{a}} = \frac{P_{e|S_{c}}(1-P_{a|S_{c}})}{P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}}}+P_{e|S_{c}}(1-P_{a|S_{c}})}\  \\
&\Pi_{\bar{c}, a} = \frac{P_{e|S_{c}}P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}}}}{P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}}}+P_{e|S_{c}}(1-P_{a|S_{c}})}
\end{split}
\right..
\end{equation}
%\setlength{\arraycolsep}{5pt}

\vspace{2mm}
\noindent\textbf{Less {\vwarm}.}
The state transition diagram of less {\vwarm} is shown in \figurename~\ref{fig:lesscase}.

When in the state $S_{c}$, {\scshape{Warm}} is not performed. The transition probability is the same as in the Conditional {\vwarm}.

  When in the state $S_{\bar{c}, a}$, {\scshape{Warm}} is performed with the probability $P_{w}^{\ominus}$.
  When the {\scshape{Warm}} is performed, the state transition is the same as in Conditional {\vwarm}.
  The state turns to $S_c$ if the {\scshape{Warm}} is performed and the {\scshape{Eviction}} event does not occur. So the probability is $P_{S_{c}|S_{\bar{c}, a}}^{\ominus}=P_{w}^{\ominus}P_{S_{c}|S_{\bar{c}, a}}=P_{w}^{\ominus}(1-P_{e|S_c})$.
   The state remains if the uncached entries are accessed during the AES execution whether the {\scshape{Warm}} is performed.
   Thus, the probability is $P_{S_{\bar{c}, a}|S_{\bar{c}, a}}^{\ominus}=(1-P_{w}^{\ominus})P_{S_{\bar{c}, a}|S_{\bar{c}, a},\bar{w}}^{\ominus}+P_{w}^{\ominus}P_{e|S_c}P_{a|S_c}$, where $P_{S_{\bar{c}, a}|S_{\bar{c}, a},\bar{w}}^{\ominus}=P_{e|S_{\bar{c}, a}}^{\ominus}P_{a|S_{\bar{c}, a}}^{\ominus}+(1-P_{e|S_{\bar{c}, a}}^{\ominus})P_{a|S_{c}}$ means the probability of state transition from $S_{\bar{c}, a}$ to $S_{\bar{c}, a}$ if {\vwarm} is not performed.
    The state turns to $S_{\bar{c}, \bar{a}}$ if the uncached entries are not accessed,
    and the probability is $P_{S_{\bar{c}, \bar{a}}|S_{\bar{c}, a}}^{\ominus}=(1-P_{w}^{\ominus})(1-P_{S_{\bar{c}, a}|S_{\bar{c}, a},\bar{w}}^{\ominus})+P_{w}^{\ominus}P_{e|S_c}(1-P_{a|S_c})$.

%
%  When {\scshape{Warm}} is not performed, whether the {\scshape{Eviction}} event occurs or not, some lookup entries are not in the cache. If these uncached entries are accessed during the AES execution, it remains the state $S_{\bar{c}, a}$, otherwise it turns to the state $S_{\bar{c}, \bar{a}}$. In summary, the state turns to $S_c$ with $P_{S_{c}|S_{\bar{c}, a}}^{\ominus}=P_{w}^{\ominus}(1-P_{e|S_c})$, remains $S_{\bar{c}, a}$ with $P_{S_{\bar{c}, a}|S_{\bar{c}, a}}^{\ominus}=(1-P_{w}^{\ominus})P_{S_{\bar{c}, a}|S_{\bar{c}, a},\bar{w}}^{\ominus}+P_{w}^{\ominus}P_{e|S_c}P_{a|S_c}$ and turns to $S_{\bar{c}, \bar{a}}$ with $P_{S_{\bar{c}, \bar{a}}|S_{\bar{c}, a}}^{\ominus}=(1-P_{w}^{\ominus})(1-P_{S_{\bar{c}, a}|S_{\bar{c}, a},\bar{w}}^{\ominus})+P_{w}^{\ominus}P_{e|S_c}(1-P_{a|S_c})$, where $P_{S_{\bar{c}, a}|S_{\bar{c}, a},\bar{w}}^{\ominus}$ means the probability of state transition from $S_{\bar{c}, a}$ to $S_{\bar{c}, a}$ if {\vwarm} is not performed.

    When in the state $S_{\bar{c}, \bar{a}}$, the {\scshape{Warm}} is not performed. Whether the {\scshape{Eviction}} event occurs or not, some lookup entries are not in the cache. So if these uncached entries are accessed during the AES execution, it turns to the state $S_{\bar{c}, a}$, and the transition probability is $P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}}}^{\ominus}=P_{e|S_{\bar{c}, \bar{a}}}^{\ominus}P_{a|S_{\bar{c}, \bar{a}}}^{\ominus}+(1-P_{e|S_{\bar{c}, \bar{a}}}^{\ominus})P_{a|S_{c}}$. Otherwise it remains the state $S_{\bar{c}, \bar{a}}$ and $P_{S_{\bar{c}, \bar{a}}|S_{\bar{c}, \bar{a}}}^{\ominus}=1-P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}}}^{\ominus}$.


We use $\Pi_{c}^{\ominus}$, $\Pi_{\bar{c}, \bar{a}}^{\ominus}$, $\Pi_{\bar{c}, a}^{\ominus}$ to represent
the stationary distribution of the three states and the values are as follows.

%\setlength{\arraycolsep}{0.0em}
\begin{equation}
\label{equ:pil}
\left\{
\begin{split}
&\Pi_{c}^{\ominus} = \frac{(1-P_{e|S_{c}})P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}}^{\ominus}P_{w}^{\ominus}}{D_L}\ \\
&\Pi_{\bar{c}, \bar{a}}^{\ominus} = \frac{P_{e|S_{c}}(1-P_{S_{\bar{c}, a}|S_{\bar{c},a},\bar{w}}^{\ominus})}{D_L} \\
&\ \ \ \ +\frac{P_{e|S_{c}}(P_{S_{\bar{c}, a}|S_{\bar{c},a},\bar{w}}^{\ominus}-P_{a|S_{c}})P_{w}^{\ominus}}{D_L}\  \\
&\Pi_{\bar{c}, a}^{\ominus} = \frac{P_{e|S_{c}}P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}}^{\ominus}}{D_L}
\end{split}
\right.,
\end{equation}
where
\begin{align}
D_L &= P_{e|S_{c}}(1+P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}}^{\ominus}-P_{S_{\bar{c}, a}|S_{\bar{c},a},\bar{w}}^{\ominus}) \nonumber \\
&\ \ +(1-P_{e|S_{c}})P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}}^{\ominus}P_{w}^{\ominus} \nonumber \\
&\ \ +(P_{S_{\bar{c}, a}|S_{\bar{c},a},\bar{w}}^{\ominus}-P_{a|S_{c}})P_{e|S_{c}}P_{w}^{\ominus} . \nonumber
\end{align}
%\setlength{\arraycolsep}{5pt}

\vspace{2mm}
\noindent\textbf{More {\vwarm}.}
The state transition diagram of more {\vwarm} is shown in \figurename~\ref{fig:morecase}.

When in the state $S_{c}$, {\scshape{Warm}} is performed with the probability $P_{w}^{\oplus}$ but has no effect.
  When in the state $S_{\bar{c}, a}$, {\scshape{Warm}} is performed loading all lookup entries into caches.
   The two situations are the same with  Conditional {\vwarm} and the transition probability is the same.

    When in the state $S_{\bar{c}, \bar{a}}$, {\scshape{Warm}} is performed with the probability $P_{w}^{\oplus}$.
     If {\scshape{Warm}} is performed, all lookup entries are in caches and the state transition is the same as in the state $S_{c}$.
      The state turns to $S_c$ with the probability $P_{S_{c}|S_{\bar{c}, \bar{a}}}^{\oplus}=P_{w}^{\oplus}P_{S_{c}|S_{c}}=P_{w}^{\oplus}(1-P_{e|S_c})$.
    If {\scshape{Warm}} is not performed, whether the {\scshape{Eviction}} event occurs or not, some lookup entries are not in the cache.
     So if these uncached entries are accessed during the AES execution, it turns to the state $S_{\bar{c}, a}$,
      and $P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}}}^{\oplus}=P_{w}^{\oplus}P_{e|S_c}P_{a|S_c}+(1-P_{w}^{\oplus})P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}},\bar{w}}^{\oplus}$.
      Otherwise it remains the state $S_{\bar{c}, \bar{a}}$ and
      $P_{S_{\bar{c}, \bar{a}}|S_{\bar{c}, \bar{a}}}^{\oplus}=P_{w}^{\oplus}P_{e|S_c}(1-P_{a|S_c})+(1-P_{w}^{\oplus})(1-P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}},\bar{w}}^{\oplus})$, where $P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}},\bar{w}}^{\oplus}$ means the probability of state transition from $S_{\bar{c}, \bar{a}}$ to $S_{\bar{c}, a}$ if {\vwarm} is not performed.

We use $\Pi_{c}^{\oplus}$, $\Pi_{\bar{c}, \bar{a}}^{\oplus}$, $\Pi_{\bar{c}, a}^{\oplus}$ to represent
the stationary distribution of the three states and the values are as follows.
%\setlength{\arraycolsep}{0.0em}
\begin{equation}
\label{equ:pim}
\left\{
\begin{split}
&\Pi_{c}^{\oplus} = \frac{(1-P_{e|S_{c}})(P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}},\bar{w}}^{\ominus}(1-P_{w}^{\oplus})+P_{w}^{\oplus})}{D_M}\ \\
&\Pi_{\bar{c}, \bar{a}}^{\oplus} = \frac{P_{e|S_{c}}(1-P_{a|S_{c}})}{D_M}\  \\
&\Pi_{\bar{c}, a}^{\oplus} = \frac{P_{e|S_{c}}P_{a|S_{c}}P_{w}^{\oplus}}{D_M} +\frac{P_{e|S_{c}}P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}},\bar{w}}^{\ominus}(1-P_{w}^{\oplus})}{D_M}
\end{split}
\right.,
\end{equation}
%\setlength{\arraycolsep}{5pt}
where
\begin{align}
D_M &= P_{e|S_{c}}+P_{w}^{\oplus}+P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}},\bar{w}}^{\ominus}(1- P_{w}^{\oplus}) \nonumber \\
&-P_{e|S_{c}}(P_{w}^{\oplus}+P_{a|S_{c}}-P_{a|S_{c}}P_{w}^{\oplus}) \ .  \nonumber
\end{align}

%\vspace{2mm}
%\noindent\textbf{3. The Best Opportunity to Perform {\vwarm} Operation.}
\subsubsection{The Best Strategy to Perform {\vwarm} Operation}
We first compare the performance of conditional {\vwarm} with less {\vwarm} and more {\vwarm} strategies applied in a large number of continuous AES encryptions.
 Followed we investigate the relationship on the performance between conditional {\vwarm} and arbitrary strategies.
  Finally, we estimate whether the conditions for the conditional {\vwarm} to get the optimal performance are satisfied in commodity computer systems.

  We denote the extra cost (execution time) introduced by {\vdelay} and {\vwarm} as
    $T_{d}$ and $T_{w}$, respectively.
$T_{d} = T_{mm}-T_{nm}$ is constant while
    $T_{w}$ varies because the number of loaded cache lines of table entries may be different when {\vwarm} is performed.


%%Then we have the following theorems.
%\vspace{2mm}
%\noindent\textbf{Comparing Conditional \vwarm~ with Less {\vwarm}.}
%By comparing the expected extra time introduced by the two strategies, we get the following theorem.
\begin{theorem}\label{lemma:warmnum1}
  The conditional {\vwarm} strategy provides better performance than the less {\vwarm} strategy.
\end{theorem}
\begin{proof}
The expected extra time introduced by less {\vwarm} and conditional {\vwarm}  are denoted as $E(T^{\ominus})$ and $E(T)$, respectively.
In the state $S_{\bar{c}, a}$, the {\vdelay} operation is needed,
so the extra time introduced by {\vwarm} is masked in the {\vdelay} operation.
 In the state $S_{c}$ and $S_{\bar{c}, \bar{a}}$, {\vwarm} and {\vdelay} are not needed for these two strategies.
The difference between the expected extra time introduced by less {\vwarm} and conditional {\vwarm} is:
%\begin{footnotesize}
\begin{equation}
\label{equ:tpc1}
E(T^{\ominus})-E(T)=\Pi_{\bar{c}, a}^{\ominus} * T_{d} - \Pi_{\bar{c}, a} * T_{d}
\end{equation}

And then, we get that $E(T^{\ominus}) > E(T)$ for all $P_{w}^{\ominus}\in(0,1)$.
So the extra time introduced by less {\vwarm} strategy is always larger than conditional {\vwarm} strategy. That is the conditional {\vwarm} strategy has better performance than the less {\vwarm} strategy.
Detailed calculation process for equation~\ref{equ:tpc1} is in Appendix~\ref{appendixa1}.
\end{proof}

%\vspace{2mm}
%\noindent\textbf{Comparing Conditional \vwarm~ with More {\vwarm}.}
%By comparing the expected extra time introduced by the two strategies, we get the following theorem.
Next, we compare the more {\vwarm} with conditional {\vwarm} strategy.
First we denote:
\begin{align}
&G = T_{d} / T_{w}\ , \nonumber \\
&M = T_{w}^{min} / T_{d}\ ,  \nonumber \\
&D_{F}=P_{e|S_{c}}(P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}}}-P_{e|S_{c}}P_{a|S_{c}})(P_{a|S_{c}}-1) \nonumber \\
   & \ \ \ \ +M(1-P_{e|S_{c}})P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}}}(P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}}}+P_{e|S_{c}}(1-P_{a|S_{c}}))\ , \nonumber \\
&F_G = \frac{P_{e|S_{c}}(P_{a|S_{c}}-1)(P_{S_{\bar{c}, a}|S_{\bar{c}, \bar{a}}}+P_{e|S_{c}}(1-P_{a|S_{c}}))}{D_{F}}\ . \nonumber
\end{align}

%Let $g = T_{delay} / T_{warm}$ and $M = T_{warm_{min}} / T_{delay}$.
%We denote $P_{evict}(P_{a}^{\mathfrak{2}}-P_{evict}P_{a}^{\mathfrak{1}})(1-P_{a}^{\mathfrak{1}})-M(1-P_{evict})P_{a}^{\mathfrak{2}}(P_{a}^{\mathfrak{2}}+P_{evict}(1-P_{a}^{\mathfrak{1}}))$ as $G_D$.
%We denote $\frac{P_{evict}(1-P_{a}^{\mathfrak{1}})(P_{a}^{\mathfrak{2}}+P_{evict}(1-P_{a}^{\mathfrak{1}}))}{G_D}$ as $G_e$.
\begin{theorem}\label{lemma:warmnum2}
%If $P_{evict}\leq \frac{M*P_{a2}}{1-P_{a1}}$,
%    conditional {\vwarm} strategy has better performance than the less {\vwarm} strategy;
%or if $P_{evict} > \frac{M*P_{a2}}{1-P_{a1}}$,
%    conditional {\vwarm} strategy has better performance than the less {\vwarm} strategy when $g<g_{max}$,
%        where
%\begin{equation}
%\label{equ:kzero}
%g_{max} = \frac{P_{evict}(1-P_{a1})(P_{a2}+P_{evict}P_{a2}+P_{evict}(1-P_{a1}))}{(P_{evict}P_{a2}+P_{a2}-P_{evict}P_{a1})(P_{evict}(1-P_{a1})-MP_{a2})-MP_{evict}P_{a2}}
%\end{equation}
% \end{theorem}
Conditional {\vwarm} strategy provides better performance than the more {\vwarm} strategy if the following condition
 is satisfied:
%\begin{itemize}
%  \item
%    $D_{F} \geq 0$ or $D_{F} < 0$ and $G < \min(F_G)$.
%\end{itemize}
    \begin{equation*}
    (1)\ D_{F} \geq 0 \ or\ (2)\ D_{F} < 0 \  and \ G < \min(F_G) .
%    \left\{
%    \begin{split}
%    &D_{F} \geq 0 \\
%    &\textbf{or}\ \   D_{F} < 0 \  and \ G < \min(F_G)
%    \end{split}
%    \right. .
    \end{equation*}

\end{theorem}

\begin{proof}
The process of proof is the same as Theorem~\ref{lemma:warmnum1} that compares $E(T)$ with $E(T^{\oplus})$.
In the state $S_{\bar{c}, a}$, the {\vdelay} operation is needed,
so the extra time introduced by {\vwarm} is masked in the {\vdelay} operation for these two strategies.
 In the state $S_{c}$ and $S_{\bar{c}, \bar{a}}$, {\vwarm} and {\vdelay} are not needed for conditional {\vwarm}.
But for more {\vwarm} strategy, {\vwarm} is performed with $P_{w}^{\oplus}$.
 Thus,
\begin{align}
\label{equ:tpc2}
&E(T^{\oplus})-E(T)=\Pi_{\bar{c}, a}^{\oplus} * T_{d} + \Pi_{c}^{\oplus}*P_{w}^{\oplus}*T_{w}^{min} \nonumber \\
 &\ \ \ \ \ \ \ \ \ + \Pi_{\bar{c}, \bar{a}}^{\oplus}*P_{w}^{\oplus}*T_{w} - \Pi_{\bar{c}, a} * T_{d}
\end{align}

Then we draw the result that $E(T^{\oplus})\geq E(T)$ if
%\begin{itemize}
  %\item
    \begin{equation*}
    (1)\ D_{F} \geq 0 \ or\ (2)\ D_{F} < 0 \  and \ G < \min(F_G) .
%    \left\{
%    \begin{split}
%    &D_{F} \geq 0 \\
%    &\textbf{or}\ \   D_{F} < 0 \  and \ G < \min(F_G)
%    \end{split}
%    \right..
    \end{equation*}
    %$\{$.
%\end{itemize}
The detailed calculation steps are in Appendix~\ref{appendixa2}.
\end{proof}

%\vspace{2mm}
%\noindent\textbf{The {\vwarm} Strategy with the Best Performance.}
%By computing the expected extra time introduced by different warm strategies, we can get the following theorem.

\begin{theorem}\label{lemma:warmnum3}
    the conditional {\vwarm} strategy that performing {\vwarm} when some cache misses occur during the previous execution of encryption results in the least extra time of {\vwarm} and {\vdelay},
if the following condition is satisfied:
%\begin{itemize}
%  \item
%    $\forall D_{F}$, when  $D_{F} < 0$, $G < \min(F_G)$.
%\end{itemize}
    \begin{equation*}
    (1)\ D_{F} \geq 0 \ or\ (2)\ D_{F} < 0 \  and \ G < \min(F_G) .
%    \left\{
%    \begin{split}
%    &D_{F} \geq 0 \\
%    &\textbf{or}\ \   D_{F} < 0 \  and \ G < \min(F_G)
%    \end{split}
%    \right..
    \end{equation*}
\end{theorem}

\begin{proof}
Firstly, we consider an extreme case that is performing {\vwarm} with a probability $P_{w}^{\odot}\in[0,1]$ just when cache misses do not occur during the pervious encryption execution, while not performing {\vwarm} when cache misses occur. This case is denoted as unusual {\vwarm}.
The state transition of unusual {\vwarm} is shown in \figurename~\ref{fig:cocase}.
%In this case, when there is a cache miss in the previous, {\vdelay} is needed. If the next encryption accesses any entry that is currently not in caches, performing {\vwarm} before hand introduces non extra overhead.
% However, if not performing {\vwarm},
%   the extra time introduced by {\vdelay} is listed in Table\ref{tbl:warmcontent}.
%  %as proved in Theorem~\ref{theorm:warmall},
%   %when some cache line size of lookup tables are not cached, not performing {\vwarm} introduces more extra time.
% Further more, complementary {\vwarm} strategy will introduce extra overhead when cache misses no occur. Therefore complementary {\vwarm} strategy has less performance than conditional {\vwarm}.
%On the other hand, the Markov state transition diagram of complementary {\vwarm} is in \figurename~\ref{fig:cocase}.
%Using the same calculation process within Theorem~\ref{lemma:warmnum1},
% we can get that conditional {\vwarm} strategy has better performance than complementary {\vwarm} strategy.
It can be proved that conditional {\vwarm} strategy has better performance than complementary {\vwarm} strategy using the same method as in Theorem~\ref{lemma:warmnum1}. The detailed calculation steps are in Appendix~\ref{appendixa}.

Generally,
 any {\vwarm} strategies can be divided into the combination of less {\vwarm}, more {\vwarm} and unusual {\vwarm}.
  We denote the arbitrary strategy as $W$. In $W$, performing {\vwarm} with a probability $P_{1}\in[0,1]$  when a cache miss occurs during the pervious encryption execution, while performing {\vwarm} with a probability $P_{2}\in[0,1]$ in other conditions.
   Similarly, we denote less {\vwarm} strategy, more {\vwarm} strategy and unusual {\vwarm} as $W_{L}$, $W_{M}$ and $W_{U}$.
  Then we can get the equation:
  \begin{equation}
W = x_{1}W_{L}+x_{2}W_{M}+x_{3}W_{U}
\end{equation}
  where $x_{1}$, $x_{2}$ and $x_{3}$ satisfy:
$$\left\{
\begin{aligned}
&x_{2}P_{w}^{\oplus}+x_{3}P_{w}^{\odot}=P_{2} \\
&x_{1}P_{w}^{\ominus}+x_{2}=P_{1} \\
&x_{1}+x_{2}+x_{3}=1
\end{aligned}
\right..
$$

In this way, the arbitrary strategy is divided. It has already been proved that conditional {\vwarm} has the best performance compared with less {\vwarm} strategy and unusual {\vwarm}. Meanwhile if satisfied the condition in Theorem~\ref{lemma:warmnum2}  conditional {\vwarm} has better performance compared with more {\vwarm} strategy.
So conditional {\vwarm} has better performance than \textcolor[rgb]{1.00,0.00,0.00}{warm strategies} with any $P_{1}$ and $P_{2}$,
if condition:
%\begin{enumerate}
%  \item
%    $GDenominator \leq 0$;
%  \item
%    or if $GDenominator > 0$, $g$ is smaller than the minimum value of $Gexpression$.
%\end{enumerate}
%\begin{itemize}
%  \item
%    $\forall D_{F}$, when  $D_{F} < 0$, $G < \min(F_G)$.
%\end{itemize}
    \begin{equation*}
    (1)\ D_{F} \geq 0 \ or\ (2)\ D_{F} < 0 \  and \ G < \min(F_G) .
%    \left\{
%    \begin{split}
%    &D_{F} \geq 0 \\
%    &\textbf{or}\ \   D_{F} < 0 \  and \ G < \min(F_G)
%    \end{split}
%    \right.
    \end{equation*}
 is satisfied.
That is  performing {\vwarm} when some cache misses occur during the previous execution of encryption results in the least extra time of {\vwarm} and {\vdelay}.
\end{proof}


%\begin{proof}
%%We cannot directly determine or predict whether the lookup tables are in the cache or not. Therefore, we can only decide whether to perform the {\vwarm}  operation based on the time of the previous execution. %As we just proved, whenever we perform \vwarm, we will load all lookup tables into cache.
%In order to prove Theorem \ref{lemma:warmaftermiss}, we compare it with two cases:
%\begin{itemize}
%  \item When there is a cache miss in the previous AES execution, we perform  {\vwarm}  with a probability less than 1.
%  \item Performing {\vwarm} operation with a probability $P_{warm}$, regardless of the cache state after the previous execution.
%\end{itemize}
%
% Denote this strategy as \emph{probabilistic \vwarm}, while our scheme as \emph{conditional \vwarm}.
%
%
%We use the Markov model to analyse both \emph{probabilistic \vwarm}~and \emph{conditional \vwarm}.
%\figurename~\ref{pic:statetrans} shown the state transition diagram.
%We use $\Pi_{full}^{p}$ ($\Pi_{full}^{c}$), $\Pi_{nonas}^{p}$ ($\Pi_{nonas}^{c}$), $\Pi_{as}^{p}$ ($\Pi_{as}^{c}$) to represent the limit distribution of the three states  when the Markov chain in stable state for the probabilistic and conditional {\vwarm}  respectively, and the values are as follows.
%
%%\begin{figure*}[t]
%%    \centering
%%    \subfloat[Our scheme]{\includegraphics[width=0.33\textwidth]{pic/conditionwarm.pdf}
%%    \label{fig:conditioncase}}
%%    \hfil
%%    \subfloat[Less warm]{\includegraphics[width=0.5\textwidth]{pic/probwarmless.pdf}
%%    \label{fig:lesscase}}
%%    \hfil
%%    \subfloat[More warm]{\includegraphics[width=0.5\textwidth]{pic/probwarmmore.pdf}
%%    \label{fig:morecase}}
%%    \caption{The Markov state-transferring probability diagram.}
%%    \label{pic:statetrans}
%%
%%\end{figure*}
%
%%%\begin{footnotesize}
%%\setlength{\arraycolsep}{0.0em}
%%\begin{eqnarray}
%%\label{equ:pip}
%%%\scriptsize
%%&\Pi_{full}^{p} = \frac{P_{warm}}{P_{evict}+P_{warm}}\ , \Pi_{nonas}^{p} = \frac{P_{nona}P_{evict}}{P_{evict}+P_{warm}}\ , \nonumber\\
%%&\Pi_{as}^{p} = \frac{P_{evict}(1-P_{nona})}{P_{evict}+P_{warm}}
%%\end{eqnarray}
%%\begin{eqnarray}
%%\label{equ:pic}
%%%\scriptsize
%%&\Pi_{full}^{c} = \frac{1-P_{nona}}{1-P_{nona}+P_{evict}}\ , \Pi_{nonas}^{c} = \frac{P_{nona}P_{evict}}{1-P_{nona}+P_{evict}}\ , \nonumber\\
%%&\Pi_{as}^{c} = \frac{P_{evict}(1-P_{nona})}{1-P_{nona}+P_{evict}}
%%\end{eqnarray}
%%\setlength{\arraycolsep}{5pt}
%%%\end{footnotesize}
%
%The expected extra time introduced by the probabilistic and conditional {\vwarm}  are denoted as $E(T^{p})$ and $E(T^{c})$, respectively.
%Then,
%%\begin{footnotesize}
%\begin{eqnarray}
%\label{equ:tpc1}
%E(T^{p})-E(T^{c})&=&\Pi_{as}^{p} * t_{delay} - \Pi_{as}^{c} * t_{delay} \nonumber \\
%&& + (1 - \Pi_{as}^{p})*P_{warm} *t_{warm}
%\end{eqnarray}
%%\end{footnotesize}
%%\begin{equation}\begin{split}
%%\label{equ:tpc2}
%%\scriptsize
%%T^{p} - T^{c} =& \frac{1}{(P_{warm}+P_{evict})(P_{delay}+P_{evict})}((P_{delay}+P_{evict})*P_{warm}^{2} + \\ & P_{evict}*(P_{delay}+P_{evict})(1-P_{delay})*P_{warm} -\\ & k*P_{delay}*P_{evict}*P_{warm}+ k*P_{delay}^{2}*P_{evict})t_{warm}
%%\end{split}\end{equation}%这个公式太长了，是不是可以在这里不写，放到附录里？
%
%We use so $E(T^{p}) > E(T^{c})$ holds when  $1 \leq k \leq k_{0}$. %from the equation we can get that when
%%$$
%%\label{equ:kres}
%%    1 \leq k \leq k_{0}
%%$$
%
%\end{proof}

%$T^{p} > T^{C}$.
%\paragraph{comparing conditional \vwarm~ with general warm strategy}
\begin{corollary}
For the AES-128 implementation with a 2KB lookup table
in commodity computer systems,
performing  {\vwarm}  when cache-miss occurs during the previous execution,
  results in the least extra time of {\vdelay} and {\vwarm}.
\end{corollary}

\begin{proof}
For the AES-128 implementation with a 2KB lookup table, $P_{a|S_c}$ is larger than 0.994 calculated from Equation~\ref{equ:delay},
and the range of $P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}}$ is $[0.994,1]$.
For the Lenovo ThinkCentre M8400t PC with an Intel Core i7-2600 CPU and 2GB RAM,
$T_{d}$ is $2479.00$ CPU cycles;
the minimum of $T_{w}$($T_{w}^{min}$) is $88$ cycles when accessing the lookup table from the L1D cache;
and the maximum of $T_{w}$ is $1745.60$ CPU cycles when all the lookup entries are not cached.
Thus, $M=0.0355$ and the rang of $G$ is $[1.42,28.17]$.


%
%In Equation~\ref{equ:kzero}, if we regard $g_{max}$ as the function of $P_{a1}$, we can get that $g_{max}$ is a monotony increase function when $P_{a1}$ is larger than 0.994.
%That is when $P_{a1} = 0.994$, $g_{max}$ gets the minimum value.
%Also if we regard $g_{max}$ as the function of $P_{evict}$, we can get that $g_{max}$ is a monotony decrease function when when $P_{a1}$ is larger than 0.994.
%In the same way, if we regard $g_{max}$ as the function of $P_{a2}$, when $P_{evict}$ is small, $g_{max}$ is a monotony increase function. But when $P_{evict}$ is large, $g_{max}$ is a monotony decrease function.
%Therefore, the minimum value of $g_{max}$ is 40.3 when $P_{evict} = 1$, $P_{a1} = 0.994$ and $P_{a2} = 1$.
%The max value of $g$ is less than the minimum value of $g_{max}$, that is $g < g_{max}$.
%So according to Theorem~\ref{lemma:warmnum3}, performing  {\vwarm}  when cache-miss occurs during the previous execution results the minimum extra time

In this environment, the range of $D_{F}$ is calculated as $[-0.000036,0.0355]$. When $D_{F} > 0$, Theorem~\ref{lemma:warmnum3} is satisfied. While $D_{F} < 0$, $F_G$ is a monotonous increasing function of $P_{a|S_c}$.
$F_G$ gets the minimum value when $P_{a|S_c} = 0.994$.
At this point, $P_{e|S_c}$ and $P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}}$ satisfy:
\begin{equation}
\label{equ:fg}
\left\{
\begin{split}
&P_{e|S_c} = \frac{0.994M(1-P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}})+\sqrt{\Delta}}{0.006(1-0.006M)} \\
&P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}} = \frac{0.07746P_{e|S_c}}{\sqrt{M(1-P_{e|S_c})}}-0.006P_{e|S_c} \\
& if\  P_{e|S_c} >1, P_{e|S_c} = 1 and P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}} = 1 \\
& if\  P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}} >1, P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}} = 1
\end{split}
\right. \ ,
\end{equation}
where
\begin{align}
\Delta = &0.988036M^2- 1.976072M^{2}P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}} \nonumber \\
&+(0.006M+ 0.988M^2)(P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}})^{2} \ . \nonumber
\end{align}

By computation we get that when $P_{e|S_c} = 1$, $P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}} = 1$ and $P_{a|S_c} = 0.994$, $F_G$ gets the minimum value.
The minimum value is $167.67$.
Therefore, $G$ is much less than the minimum value of $F_G$ when $D_F < 0$.
 So based on Theorem~\ref{lemma:warmnum3}, performing  {\vwarm}  when cache-miss occurs during the previous execution,
  results in the least extra time of {\vdelay} and {\vwarm}.
\end{proof}

%1.对极值点和极值的影响
For different platforms, $T_{w}$ and $T_{d}$ are different. So the value of $M$ and the rang of $G$ are different.
 This may leads to the change of the minimum value of $F_G$.
  Through the analysis of Equation~\ref{equ:fg}, when $M \geq 0.0069$, $P_{e|S_c} >1$.
   In this case, $F_G$ has the same minimum value when $P_{e|S_c} = 1$, $P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}} = 1$ and $P_{a|S_c} = 0.994$.
    While when $M < 0.0069$, the minimum point($P_{e|S_c}$, $P_{S_{\bar{c}, a}|S_{\bar{c},\bar{a}}}$,$P_{a|S_c}$) may be different.
     And the minimum value is less than $167.67$.

%2.对G的范围的影响
The maximum value of $G$ is $1/M$. When $M\geq 0.0069$, $1/M\leq 144.93$. This value is less than $167.67$, so the condition in Theorem~\ref{lemma:warmnum3} is satisfied which means performing  {\vwarm}  when cache-miss occurs during the previous execution,
  results in the least extra time of {\vdelay} and {\vwarm}.
    When $M < 0.0059$, the maximum value of $G$ is larger than $169.49$.
     Thus the condition in Theorem~\ref{lemma:warmnum3} is not hold any more. In this case, the more {\vwarm} strategy has better performance.
      This is reasonable because the cost of {\vwarm} is very little here.
  While when $ 0.0059 \geq M < 0.0069$, whether the conditional {\vwarm} has better performance depends on the minimum value of $F_G$.

%\qed
%\end{proof}


%\begin{table*}[t]%[!hbt]
%  \centering
%  \small
%  \caption{The values of $k_{0}$ corresponding to $P_{evict}$. ($P_{delay} = 0.994$)}\label{tbl:kvalue}
%   \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
%   \hline
%   {$P_{evict}$} & 0   & 0.05  & 0.1 & 0.2   & 0.3   & 0.4   & 0.5  & 0.6   & 0.7    & 0.8  & 0.9 & 1 \\
%   \hline
%   {$k_{0}$} & $\infty$ & 3502.1 & 1835.4 & 1002.2 & 724.5 & 585.7 & 502.5 & 447.1 & 407.5 & 377.8 & 354.8 &336.3 \\
%   \hline
%   \end{tabular}
%\end{table*}

%In the above proof, we treated AES as one atomic operation and excluded some special situations. In reality, the AES execution can be interrupted. We did not consider the cases such as when the process scheduling and interruption occur during the AES. These conditions will be discussed later.%这里我是想说，在上面的这个证明里面，我们将AES过程当做了一个黑盒子，也就是一个原子操作。因此，我们有两种情况没有考虑，1是没有考虑中断或者调度发生在AES过程中，2是没有考虑周期调度函数执行，但并未发生调度。这两个情况将会在后面进行论证。在这里需要说明一下么？
%\subsection{Remarks}
%\vspace{2mm}
%\noindent\textbf{4. Remarks.}
\subsubsection{Remarks}
\label{remark}
In the analysis of {\vwarm} operation before, we assume that some conditions are not considered. In this part, we discuss these conditions.
First is the effect of partial warm caused by the AES execution. Second is the influence of process scheduling and interruptions.
Finally, we summarize the relation between the measured time and the cache states.
%$S_{\bar{c}, a}$ and $S_{\bar{c}, \bar{a}}$

%\noindent\textbf{One AES execution is a partial warm operation.}
%\subsubsection{One AES execution is a partial warm operation}
\vspace{2mm}
\noindent\textbf{One AES Execution Is a Partial Warm Operation.}
When the system is in the state $S_{\bar{c}, a}$, one AES execution is a partial warm operation. In this case, cache misses are resulted to load the corresponding entries into the cache.

%下面注释掉的这段是之前的证明，但是现在看来似乎是不需要了。
%Here, we prove that the extra time caused by not performing the \verb+warmup+ operation after an AES execution at the state $S_{as}$, is larger than our scheme. The probability that the next consecutive AES execution needs \verb+delay+ when $N$ cache line size of the lookup tables are in the cache before the AES execution (i.e.,  partial \verb+warmup+) is shown in Equation~\ref{equ:delay}, while the introduced extra time $T_{delay,N} = P_{delay,N} * t_{delay}$. The extra time $T_{warm,N}$ for our scheme is used to access all the lookup tables, including the ones in and not in the cache, and is shown in Equation~\ref{equ:partwarm}, where $t_c$ ($t_{nc}$) is the time for accessing one cache line that is in (not in) the cache. Evaluations on different platforms show that  $T_{delay,j} > T_{warm,j}$ for $j = 0, 1, ..., 67$.
%For example, in the environment described in Section~\ref{sec:eval}, $t_c = 5.01$, $t_{nc}=36.62$, $t_{delay}=4287$, the $T_{delay,j}$  and $T_{warm,j}$ are listed in Table~\ref{tbl:partwarm}.
%
%\begin{equation}
%\label{equ:partial}
%P_{delay,j} = \sum_{i=0}^{68-j}C_{68-j}^{i}(1-P_e)^{i}P_e^{68-j-i}(1-P_e^{68-i-j})
%\end{equation}
%\begin{equation}
%\label{equ:partwarm}
%T_{warm,j} = \sum_{i=0}^{68-j}C_{68-j}^{i}(1-P_e)^{i}P_e^{68-j-i}((i+j)t_c+(68-i-j)t_{nc})
%\end{equation}
%
%
%\begin{table}[!hbt]
%  \centering
%  \small
%  \caption{$T_{delay,j}$  and $T_{warm,j}$ (in ms).}\label{tbl:partwarm}
%   \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
%   \hline
%   {j}           & 0      & 1  & 2    & 10   & 20   & 30   & 40  & 50   & 60    & 67\\
%   \hline
%   {$T_{delay,j}$} & 4276.14 & 4275.14 & 4274.05 & 4260.84 & 4223.99 & 4135.21 & 3921.36 & 3406.20 & 2165.23 & 360.81 \\
%   \hline
%   {$T_{warm,j}$} & 535.11 & 532.18 & 529.26 & 505.60 & 474.86 & 440.87 & 398.26 & 332.83 & 206.97 & 34.42 \\
%   \hline
%   \end{tabular}
%\end{table}

%In our analysis, the warmup operation is performed after the AES execution. When doing the delay operation, the time consumption of the warmup operation is included in the delay time. So it is better to perform warmup when need to delay because the warmup cost nothing at this time.%这段话是想说，warm是在delay之后做的，而且warm的消耗包含在了delay的消耗里，所以只要需要进行delay的时候，进行一次warm是一个更好的选择，因为这个时候warm是不会产生额外消耗的。这段话放在这里似乎不太合适，但我没想好放在哪里。

When the system state is $S_{\bar{c}, a}$, conditional {\vwarm} strategy performs {\vwarm}  with probability 1,
 while the general warm strategy performs {\vwarm} with a probability $P_{1}^{'}\in[0,1]$ greater than $P_{1}$ (in Theorem~\ref{lemma:warmnum3}), due to the partial warm by the AES execution.
%, it mean the AES expands the $P_{warm}$.
However, the condition that makes $E(T^{\oplus}) > E(T)$ hold is not changed.
 Moreover based on Theorem~\ref{lemma:warmnum3}, although $P_{1}^{'}$ is different, the general warm strategy still can be divided.
Therefore, conditional {\vwarm} strategy still  has better performance.


%the value of $P_{warm}$ doesn't affect the result. Although the AES expends the probability of warm, our scheme is better.
%When in the state $S_{as}$, for our conditional warmup method, we perform the warmup method. For probabilistic warmup method, originally performing the warmup operation is in a probability of $P_{warm}$. But now as the AES is a partial warmup operation, it mean the AES expands the $P_{warm}$. However, in the proof of Theorem 4, the value of $P_{warm}$ doesn't affect the result. Although the AES expends the probability of warm, our scheme is better.%这段话就是在解释AES本身是一个部分warm的过程。但是在现在的方案中，这个部分warm的效果，在我们的方案中没有体现，而在概率warm的情况下，效果就是提高了warm的概率。而我们在上面的证明过程中，得到了我们方案最优，是和概率warm的大小无关的，也就是说，这种情况下还是我们的方案更优。
%之前做的证明，是因为当时的方案是warm是在AES之前进行，所以warm执行不执行是会有时间开销的，所以是对warm开销和delay开销的一个比较。但现在，warm是没有开销的。所以把这段注释掉了。

%\noindent\textbf{Process scheduling and interruption occur during the AES execution.}
%\subsubsection{Process scheduling and interruption occur during the AES execution}
\vspace{2mm}
\noindent\textbf{Process Scheduling and Interruption Occur during The AES Execution.}
In this case,  the time of {\scshape{Encrypt}} is larger than $T_{mm}$ (i.e.$t2 - t1 > T_{mm}$), {\vwarm}  will be triggered in our scheme. %As proved in Theorem 2, we don't perform delay operation. So if we perform the warmup operation, it will bring extra overhead.
As the process scheduling and interruption occur, we assume that $N$ cache lines of the lookup table are evicted from caches.
%We assume that after this AES, there are $N$ cache line size of lookup tables not in the cache.
Hence, the extra overhead introduced by using {\vwarm}  to load the lookup table is $E(T_{w},N) = N T_{cl} + (32-N)*T_{c}$, where $T_c$ and $T_{cl}$ denote the time for accessing one cache line from caches and RAM, respectively. If we do not perform \vwarm , the next round of AES may need to access part(s) of the lookup table from RAM instead of caches, which will trigger \vdelay. The expected overhead is $D_{nl}(N)$ (Equation~\ref{equ:tdelay}).
Table~\ref{tbl:scheduaes} shows that performing {\vwarm} achieves better performance when $N>0$. If $N = 0$, it means that the lookup table is not evicted from the cache when the process scheduling and interruption occur. In this case {\vwarm} is not needed. However from the overall execution we cannot distinguish whether the lookup table is evicted, otherwise we need more privileged operations. Therefore, we have to perform {\vwarm} and thus it introduces some extra overhead.
%If not performing the warmup operation, the probability of needing delay is $P_{delay}$, and the extra time introduced is $T_{delay}$ as shown in Equation~\ref{equ:delay} and Equation~\ref{equ:tdelay}.
%So from Table~\ref{tbl:scheduaes}, we find that in this case, performing warmup operation is better.%这段是说明了调度和中断发生在AES执行过程中的情况。这时的warm是有额外开销的，比较这时使用warm的开销和不使用warm时会产生的delay的开销。得出此时使用warm是更好的。

\begin{table*}[t]
  \centering
  %\small
  %\scriptsize
  %\footnotesize
  \caption{The introduced time by performing warmup operation or not (in CPU cycles).}\label{tbl:scheduaes}
   \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
   \hline
   {N}           & 1      & 2  & 3    & 5   & 10   & 15   & 20  & 25   & 30  &31  & 32\\
   \hline
   %{$D_{nl}(N)$} & 2509.29 & 2524.92 & 2524.99 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 & 2525.00 \\
   {$D_{nl}(N)$} & 2463.58 & 2478.92 & 2478.99 & 2479.00 & 2479.00 & 2479.00 & 2479.00 & 2479.00 & 2479.00 & 2479.00 & 2479.00 \\
   \hline
   {$E(T_{warm})$} & 139.80 & 191.60 & 243.40 & 347.00 & 606.00 & 865.00 & 1124.00 & 1383.00 & 1642.00 & 1693.80 & 1745.60 \\
   \hline
   \end{tabular}
\end{table*}

%\noindent\textbf{Periodical schedule function is called without scheduling during AES execution.}
%\subsubsection{Periodical schedule function is called without scheduling during AES execution}
\vspace{2mm}
\noindent\textbf{Periodical Schedule Function Is Called without Scheduling during The AES Execution.}
As no other process invoked, the lookup table is not evicted.
Therefore, {\vwarm}   is unnecessary although $T_{nm} < t2 - t1 \leq T_{mm}$.
However, {\vwarm}   is a better choice, as we cannot predict whether the lookup table is in the cache, and the overhead introduced by accessing the elements in caches is concealed by \vdelay.

%In this case, the lookup tables normally are still in the cache. So the warmup and delay operation are not needed. Actually, in this case, $T_{nm} < end - start \leq T_{mm}$. However, we can't distinguish it is in this case or cache misses occurs. So we must do the delay operation. Moreover, in this case, performing warmup operation introduces none extra time. Therefore, we can regard this case as in the state $S_{as}$. %这段话是说明了周期调度函数调用但未有其他函数被调度时，AES时间是小于WET的，但是我们通过观察无法分辨是这种情况发生，还是是有cache miss发生。因此我们要对这种情况进行delay+warm的操作。其实对于概率warm的情况，也是要这么处理，所以，我们的方案合适。

%\noindent\textbf{The relationship between the cache state and execution time.}
%\subsubsection{The relationship between the cache state and execution time}
\vspace{2mm}
\noindent\textbf{The Relationship between The Cache State and Execution Time.}
In the above, we prove that performing  {\vwarm}  when not all lookup entries are in caches, results the least extra time.
However, in our {\vwd} scheme, we perform the {\vwarm} operation according to the previous AES execution time,
 as we are technically unable to observe the cache state without introducing additional overhead.
% \footnote{Otherwise, we perform  the warmup operation before each execution, which is less efficient as proved in Theorem 4.}.% 这个脚注是什么意思？为什么？
The relationship between the cache state and execution time is as follows:
\begin{itemize}
  \item $t2 - t1 \leq T_{nm}$, the system is in state $S_{c}$ or $S_{\bar{c},\bar{a}}$, no {\vwarm} is needed according to Theorem~\ref{lemma:warmnum3}.
  \item $t2 - t1 > T_{mm}$, the system is in state $S_{c}$, $S_{\bar{c}, a}$ or $S_{\bar{c}, \bar{a}}$, {\vwarm}   is needed according to Theorem~\ref{theorm:warmall}.
 % another process is scheduled during the AES execution which makes part of the lookup tables  evicted from the cache.
  \item $T_{nm} < t2 - t1 \leq T_{mm}$, %all the lookup tables may be in the cache or not, which is related with the workload in parallel.
%      The OS invokes the \verb+Scheduler()+ function every $t_{sched}$ interval and evicts at least one  entry of the lookup tables for anther process with the probability $P_{evict}$. Then, in this case,
%      when performing \verb+warmup+ for the AES execution next to the current one, the expected extra time is $T_{warm}^{t-w} = ( \frac{encEnd_c-encStart_c}{t_{interval}}*(1-P_{evict})+ \frac{encStart_c-encStart_p}{t_{interval}}*P_{evict}) * t_{warm}$;
%       while not performing the \verb+warmup+, the expected extra time is $T_{delay}^{t-w} = \frac{encStart_c-encStart_p}{t_{interval}}*P_{evict} * t_{delay}$. When $P_{evict}$ satisfies Equation~\ref{equ:con}, $T_{warm}^{t-w} < T_{delay}^{t-w}$, which means the \verb+warmup+ operation is not needed when the workload is small. In our evaluation environment, we find that when $P_{evict} < 0.18$, the \verb+warmup+ operation is unnecessary.
    %all the lookup tables may be in the cache or not.
    the system is in the state $S_{c}$ or $S_{\bar{c}, a}$, {\vwarm} is better according to previous analysis. %the periodic schedule function is called but scheduling not occurs during AES execution. But we treat them all as in the state $S_{as}$. On the above we already explain it reasonable.%这里就是说，这里对应两种情况，但是都只能当S_{as}这个状态来处理，但是之前已经说明过了，这样是没问题的。
\end{itemize}

As all situations are discussed before, according to the previous AES execution time to perform {\vwarm} is reasonable to get the best performance.


%\begin{figure}
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=0.45\textwidth]{pic/statusandtime.pdf}\\
%  \caption{The consecutive AES execution.}\label{pic:status}
%\end{figure}

%Moreover, as proved in Lemma 3, the \verb+warmup+ operations  are necessary  to provide better performance in the following two  cases where  not all the lookup tables are in the cache:
%\begin{itemize}
%  \item Before the first AES execution, where none of the lookup tables are in  cache.
%  \item The interval between two AES executions is too large or perform AES discontinuously. %在AES不连续的时候，建议进行warm操作。
%\end{itemize}

\subsection{Different Key Lengths and Implementations}
%\vspace{-1mm}
\label{analy:otherkey}
%shall be discussed here!
%++++++++++
%Table~\ref{tbl:pdelayvalue} shows the minimum value of the $P_{delay}$ in different AES key lengthes and different AES implementations. The formulas of $P_{delay}$ is in the appendix.

\begin{table}[t]
%\vspace{-2mm}
  \centering
  \small
  \caption{the minimum value of the $P_{a}$ in different cases.}
  \label{tbl:pdelayvalue}
   \begin{tabular}{|c|c|c|c|c|}
   \hline
   {table size}           & AES-128      & AES-192 & AES-256   \\
   \hline
   {5KB} & 0.85 & 0.88 & 0.90  \\
   \hline
   {4.25KB} & 0.907 & 0.944 & 0.966 \\
   \hline
   {4KB} & 0.924 & 0.954 & 0.973  \\
   \hline
   {2KB} & 0.994 & 0.998 & 0.999  \\
   \hline
   \end{tabular}
\end{table}

All the theorems above still stand, with the only difference being the value of $P_{a}$, $T_{w}$ and $T_{d}$.
The {\scshape{Warm+Delay}} scheme provides the optimal performance for various implementations of AES~\cite{openssl,polarssl} with different key length, if the conditions in previous theorems are satisfied.
$P_{a}$ (detailed in Appendix~\ref{appendixb}) is the probability that some cache line size of lookup tables not in the cache are accessed. It depends  on the structure, the size of lookup tables and the number of iterated rounds.
For mbed TLS-1.3.10~\cite{polarssl} and OpenSSL-0.9.7i~\cite{openssl}, they have 5 lookup tables with the total size 4.25KB and 5KB. For OpenSSL-1.0.2c~\cite{openssl}, the size of lookup tables is 4KB or 2KB. The number of rounds is 10, 12 and 14 for AES-128, AES-192 and AES-256,respectively.
Table~\ref{tbl:pdelayvalue} lists the corresponding minimum $P_{a}$.
$T_{w}$ and $T_{d}$ are related to not only the different implementations and key length, but also the platform that running AES.
Therefore, for different implementations and key length of AES, first we should determine $P_{a}$, $T_{w}$ and $T_{d}$. Then we can use the previous theorems to determine whether the {\scshape{Warm+Delay}} scheme provides the optimal performance.
 Furthermore, the {\vwarm} and {\vdelay} can be configured to satisfy the theorems then get the optimal performance.


%We analyze three different AES implementations. The difference is in the last round of the AES. The first one uses a 256B table in the last round~\cite{polarssl}. The second one uses one 1KB table in the last round~\cite{openssl}(Openssl-0.9.7i), and the third one uses the four tables in the previous rounds~\cite{openssl}(Openssl-1.0.2c). Also we analyze the different key length of AES. AES-128 has 10 rounds, AES-192 has 12 round, and AES-256 has 14 rounds.%这是说，有三种AES实现，一种是使用5个查找表，前面几轮使用4个1K的表，最后一轮使用256B的一个表。一种是使用5个表，前几轮也是使用4个1K的表，但最后一轮使用1个1K的表。还有一种是使用4个1K的表，每轮都是使用这4个表。同时每种实现有分为三种密钥长度，每种密钥长度其实对应的是AES不同的轮数.

%For all the situations, the process of the proof of the Theorem 3 and Theorem 4 is almost the same. The main difference is the value of $P_{delay}$. We compute the value of $P_{delay}$ in the Table~\ref{tbl:pdelayvalue}, and find out that the Theorem 3 and Theorem 4 still hold true. %这段是说，对于这些不同的情况，在证明Lemma 3 and Lemma 4的时候，证明方法都是一样的，不同的就是P_{delay}的取值范围不同，但是经过计算，在每种取值范围内，Lemma 3 and Lemma 4都是成立的，也就是都是我们的方案最优。

For other table-lookup block ciphers, first it has to determine $P_{a}$, $T_{w}$ and $T_{d}$ according to the algorithm, the implementation and the running platform.
Then following our analysis before the conditions can be obtained for {\scshape{Warm+Delay}} scheme to get the optimal performance.
%Once the conditions in these theorems are satisfied, {\scshape{Warm+Delay}}  provides the optimal performance.

%it is similar to the AES. They just have different values of $P_{delay}$. The three states of the cache is same and they have the same state transition diagram. So it can be proved that the Theorem 3 and Theorem 4 hold true for them.% 这段是说，对于其他算法，Lemma 3 and Lemma 4 的证明也是类似的，同样对于cache有这三种状态，且状态转移图也是一样的，故而证明过程也一样，只是P_{delay}不同而已，可以证明对于其P_{delay}的范围，Lemma 3 and Lemma 4 也是成立的。



