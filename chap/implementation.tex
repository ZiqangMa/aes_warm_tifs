\section{Implementation and Performance Evaluation}
\label{sec:implementationEvalution}
%更多的细节考虑，应该放在这里
%\vspace{-1mm}
We evaluate the  scheme on a Lenovo ThinkCentre M8400t PC with an Intel Core i7-2600 CPU and 2GB RAM.
 This CPU has $4$ cores and each core has $32$KB L1 data caches,
  and the operating system is 32-bit Linux with kernel version 3.6.2.

\subsection{Implementation}
\label{sec:implementation}
%\vspace{-1mm}
% AES  mbed TLS-2.4.2
%In this section,
We apply the {\scshape{Warm+Delay}} scheme to AES-128 implemented with a 2K lookup table, %to validate the security and evaluate the performance.
  which is directly borrowed from OpenSSL-1.0.2c \cite{openssl}.
As we attempt to optimize performance while eliminate remote cache timing side channels,
 efficient {\scshape{Warm}}, {\scshape{GetTime}},  and {\scshape{Delay}}
  are finished;
and the constant parameters ($T_{nm}$ and $T_{mm}$) are determined properly.
 Next, we show the implementation details about the {\scshape{Warm+Delay}} scheme.


\vspace{1mm}\noindent\textbf{{\scshape{Warm}}.}
It loads all entries of the lookup table into the L1D cache
   by accessing one byte of each block of 64 bytes (i.e., the size of one cache line).
In order to ensure these operations are not obsoleted
  due to the compiler optimization,
 the variables are declared with the keyword \verb+volatile+.

%\vspace{1mm}\noindent\textbf{Time variations with cached data.}
%As mentioned above, we use the AES-128 implementation of OpenSSL-1.0.2c.
However, even when all lookup tables are in the L1D cache,
  the execution time of encryption still has variations and these variations could be exploited to launched side-channel attackers~\cite{Canteaut2006Understanding,Bernstein2005Cache}.

%There time variations result from the cache hierarchy of Intel CPUs.
%cache line; cache bank; cache set.???
%
%Each bank serves only one request at a time,
% and multiple accesses to a same cache bank  .
%
%We disable Hyper-Threading to ensure
%{\color{red}is it useful? any influence from other core?}
The time variations result from accessing the L1D cache.
There are two possible reasons. One is the cache bank conflict.
Cache bank is a physical structure that divides the L1D cache into several banks.
Each core has its own L1D cache and the banks are not across the L1D cache.
Different banks can be concurrently accessed,
but one bank only serves one request at a time.
%Intel introduced a cache design consisting of multiple banks.
%Each of the banks serves part of the cache line specified by the offset in the cache line.
%The banks can operate independently and serve requests concurrently.
%But each cache bank can only serve one request at a time.
So, multiple access operations to the same cache bank are slower than to different banks.
 Because the L1D cache is unshared, the L1D cache can not be accessed by the other cores on the same CPU,
 we disable the Hyper-Threading of the system to ensure the protected process itself not to produce concurrent access to the L1D cache for the cache bank conflict.
All the following experiments, Hyper-Threading is disabled.

The other is read-write conflict that the load from the L1D cache takes slightly more time if it involves the same set of cache lines as a recent store.
\figurename~\ref{fig:fullwarmworst} shows the distribution of the AES execution time for $2^{30}$ random plaintexts when this conflict occurs.
We avoid the read-write conflict by exploiting the stack switch technique~\cite{guan2014copker}. The aligned consecutive lookup table distributes in 32 cache sets due to the cache mapping rule while the total number of cache sets is 64 on our platform.
We declare a 2KB global array as the stack, with which we can easily control the address.
The beginning address of the array is made next to the lookup table module 4096, and this make the intermediate variables of AES execution use the remaining cache sets compared with the lookup table.

Finally,
 the distribution of the AES encryption time is shown in \figurename~\ref{fig:fullwarmbest}.
%Served as a contrast, the Figure~\ref{fig:fullwarmworst} shows the distribution that the first address of the array is the same as the lookup table module 4096.
%\begin{figure}[t]
%\centering
%%\subfigure[different inputs]{
%%\centering
%%\begin{minipage}[b]{0.6\textwidth}
%\includegraphics[width=0.4\textwidth]{pic/TWOCM_allwarm.pdf}
%%\includegraphics[width=2.5in]{pic/TWOCM_allwarm.pdf}
%%\end{minipage}
%%}
%%\subfigure[same inputs]{
%%\centering
%%\begin{minipage}[b]{0.6\textwidth}
%%\includegraphics[width=1\textwidth]{pic/fullwarmsin.pdf}
%%\label{fig:fullwarmese-sameinput}
%%\end{minipage}
%%}
% \caption{The distribution of AES encryption time in full warm condition.} \label{fig:fullwarmese-diffinput}
%\end{figure}

%\begin{figure}[t]
%\centering
%\subfigure[the worst case]{
%\centering
%\begin{minipage}[b]{0.5\textwidth}
%\includegraphics[width=\textwidth]{pic/fwopensslworst.pdf}
%\label{fig:fullwarmworst}
%\end{minipage}
%}
%\subfigure[the best case]{
%\centering
%\begin{minipage}[b]{0.5\textwidth}
%\includegraphics[width=\textwidth]{pic/fwopensslbest.pdf}
%\label{fig:fullwarmbest}
%\end{minipage}
%}
% \caption{The distribution of AES encryption time with 2KB lookup table in full warm condition.}
%\end{figure}



\begin{figure}[!t]
%\centering
    \begin{minipage}[t]{0.5\linewidth}%设定图片下字的宽度，在此基础尽量满足图片的长宽
    %\centering
    \includegraphics[width=\textwidth]{pic/fwnormal.pdf}
    \caption*{(a) in normal case}%加*可以去掉默认前缀，作为图片单独的说明
    \label{fig:fullwarmworst}
    \end{minipage}
    \begin{minipage}[t]{0.5\linewidth}%需要几张添加即可，注意设定合适的linewidth
    %\centering
    \includegraphics[width=\textwidth]{pic/fwbest.pdf}
    \caption*{(b) the best case}
    \label{fig:fullwarmbest}
    \end{minipage}
    \caption{The distribution of AES encryption time with 2KB lookup table in full warm condition.}%n张图片共享的说明
%\centering
%\subfloat[in normal case]{\includegraphics[width=2.8in]{pic/fwnormal.pdf}
%\label{fig:fullwarmworst}}
%\vfil
%\subfloat[the best case]{\includegraphics[width=2.8in]{pic/fwbest.pdf}
%\label{fig:fullwarmbest}}
%\caption{The distribution of AES encryption time with 2KB lookup table in full warm condition.}\label{fig:sim}
\end{figure}



%In our environment, for each of $2^{30}$ random plaintexts, we perform the encryption after loading  all the lookup tables into cache. Figure~\ref{fig:fullwarmese-diffinput} describes the distribution of encrypting time.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.4\textwidth]{pic/TWOCM_onenotin.pdf}\\
  %\includegraphics[width=2.5in]{pic/TWOCM_onenotin.pdf}\\
  \caption{The distribution of AES execution time only  not warm one cache line.}\label{fig:twocm}
\end{figure}


\vspace{1mm}\noindent\textbf{{\scshape{GetTime}}.}
%The function in Listing 1.1 returns the current time in high precision (e.g. in clock cycles) with low cost.
We adopt the instruction \verb+RDTSCP+ to implement {\scshape{GetTime}}, to obtain the current time in high precision (clock cycles) with low cost.
%The instruction \verb+RDTSCP+ has high precision (in clock cycle) and is adopted in our implementation.
\verb+RDTSCP+ is a serializing call which prevents the CPU from reordering it.
In the implementation, we need to perform the following operations to achieve the high accuracy:
(1) as the TSCs on each core are not guaranteed to be synchronized, we install the patch [x86: unify/rewrite SMP TSC sync code] to synchronize the TSCs;
(2) the clock cycle changes due to the energy-saving option of the computer, we  disable this option in BIOS to ensure the clock cycle be a  constant.

Besides, the cost of \verb+RDTSCP+ is 36 cycles which is much greater than the comparison operation and cannot be ignored.
so,  we perform {\scshape{GetTime}} only if $t2 - t1 < T_{mm}$,
   instead of every time after {\scshape{Warm}} (see Line 7 in Algorithm \ref{alg:aesdefense}).

\vspace{1mm}\noindent\textbf{{\scshape{Delay}}.}
The system functions such as \verb+nanosleep()+ and \verb+usleep()+,
   do not satisfy the resolution requirement,
   and they switch the process state to {TASK\_INTERRUPTIBLE}, which may cause the lookup tables to be evicted from caches.
On the contrary,
   we implement the {\vdelay} operation
 by executing XOR repeatedly as follows, achieving a high resolution without modifying the cache state.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.5\textwidth]{pic/delay.pdf}\\
    \caption{AES execution performance in different scenarios.}
    \label{pic:delay} %% label for entire figure
\end{figure}

We measure the time for different loop numbers of the \verb+xor+ instruction, by invoking it $10^6$ times with different loop numbers (from 0 to 950 and the step is 50), as shown in \figurename~\ref{pic:delay}.
The relation between the time delayed ($T_d$) and the loop number of \verb+xor+ instruction ($n$) is calculated through the least squares method. The result is $t_{delay} = 2.9886n + 15.68$, and the coefficient of determination is 0.99996.
The precision is 3 cycles, much less than the noise of remote environments, so it cannot be exploited.
Implementation of {\scshape{Delay()}} is provided in Listing 1.
We do not use a table of $t_{delay}$ and $n$ with the purpose of reducing the use of caches in our scheme.



%(3) using \verb+RDTSC+ alone will not prevent the CPU from reordering it, we add \verb+CPUID+ instruction, a serializing call,   before \verb+RDTSC+ to avoid the reordering.
%\verb+RDTSCP+, a new Intel instruction can be directly adopted without adding \verb+CPUID+ instruction,  with the same precision and accuracy as \verb+RDTSC+, but a lower cost.

%\lstset{
%basicstyle=\small\ttfamily,
%numbersep=5pt,
%xleftmargin=20pt,
%frame=tb,
%framexleftmargin=20pt
%}

%\renewcommand*\thelstnumber{\arabic{lstnumber}:}
%\begin{lstlisting}[caption={{GetTime function}}]
%inline uint64_t GetTime()
%{
%  unsigned long high, low;
%  asm volatile ("rdtscp" : "=high" (high),
%                 "=low" (low) : : "ebx", "ecx");
%  return high | ((uint64_t) low << 32);
%}

%\end{lstlisting}


%x86: unify/rewrite SMP TSC sync code

\lstset{
language=[ANSI]C,
basicstyle=\small\ttfamily,
numbersep=5pt,
xleftmargin=2pt,
breaklines=true,
%frame=tb,
frame=shadowbox,
framexleftmargin=2pt
}

\renewcommand*\thelstnumber{\arabic{lstnumber}:}

\begin{lstlisting}[caption={{The implementation of {\scshape{Delay()}}.}}]
volatile int delay(uint64_t t_delay){
    uint64_t n = (double)t_delay>15.68 ? (uint64_t)((double) t_delay/2.9886-5.2466) : 0;
    for (; n>0; n--)
        asm volatile ("xor %%eax, %%eax;" : : : "%eax");
}
\end{lstlisting}


\vspace{1mm}\noindent\textbf{$\mathbf{T_{nm}}$.}
$T_{nm}$ is larger than the minimum AES execution time (no cache miss occurs), which avoids the unnecessary {\scshape{Warm()}} and {\scshape{Delay()}} operations;
and less than the AES execution that only one cache miss occurs.
The average minimum AES execution time is measured by average $2^{30}$ AES execution time with the lookup tables all in L1D cache.
In our environment it is $331$ cycles.
 \figurename~\ref{fig:twocm} shows the distribution of AES execution time for $2^{30}$ plaintexts
   while all lookup tables except one block of 64 bytes (i.e., one cache line) are loaded in L1D cache.
Note that, this uncached entry may be unnecessary in an execution of AES encryption.
We use the stack switch technique to eliminate the read-write conflict, at the same time it makes the AES execution time much more concentrated as shown in \figurename~\ref{fig:sim}.
 This helps the determination of $T_{nm}$ can be more accurate and reasonable.
  Also we should choose the value as large as possible to avoid the influence of the fluctuation around $T_{nm}$ that might occur.
Finally, we choose 355 cycles as $T_{nm}$.
This value is chosen to ensure that all tables are in L1D caches and no fluctuation occurs around it. So no useful observation will be obtained by attackers, and no useful variations will be magnified.


\vspace{1mm}\noindent\textbf{$\mathbf{T_{mm}}$.}
$T_{mm}$ is the maximum encryption execution time when all lookup tables are out of caches and unrelated to the cache states.
Before measuring, we flush both the data and instructions out of L1/2/3 caches.
In actual measurement, we repeat the measurement for 10000 times, and calculate the average value of 100 greater measurements as $T_{mm}$.
 In this case $T_{mm}$ is 2834 CPU cycles.

 Noted that we should consider the effect of the instruction caches.
  The implementation of block ciphers is not subject
   to timing side-channel attacks based on instruction caches \cite{Onur2007Yet},
  because there is generally no branches in the execution path.
However, the instructions of block ciphers
  may be evicted from the caches due to system activities,
   which causes instruction cache misses and increases the execution time.
As the effect of instruction caches on the execution time is almost indistinguishable from that of data caches,
  we determine $T_{nm}$ when all instructions of encryption are cached
and $T_{mm}$ as the execution time
  when all instructions are not in instruction caches (and all lookup tables are not in data caches).
Otherwise,
 the measured time might leak some information about data cache access.



